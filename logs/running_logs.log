[2024-08-03 02:50:23,906]: INFO: main Welcome to out custom log
[2024-08-03 19:24:57,856]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-03 19:24:57,856]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-03 19:24:57,859]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-03 19:24:57,859]: INFO: common Created directory at: artifacts
[2024-08-03 19:24:57,859]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-03 19:25:01,368]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 9694:3AC864:672058:752C69:66AEB630
Accept-Ranges: bytes
Date: Sat, 03 Aug 2024 23:24:59 GMT
Via: 1.1 varnish
X-Served-By: cache-bos4652-BOS
X-Cache: HIT
X-Cache-Hits: 1
X-Timer: S1722727499.992293,VS0,VE64
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: 0ec71c0de26239c09116c058c9e5e77fedb4dc77
Expires: Sat, 03 Aug 2024 23:29:59 GMT
Source-Age: 0


[2024-08-03 19:25:01,521]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-13 00:08:34,993]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:08:35,011]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:08:35,013]: INFO: common Created directory at: artifacts
[2024-08-13 00:08:35,014]: INFO: common Created directory at: artifacts/data_validation
[2024-08-13 00:20:28,412]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-13 00:20:28,412]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:20:28,412]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:20:28,412]: INFO: common Created directory at: artifacts
[2024-08-13 00:20:28,412]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-13 00:20:29,314]: ERROR: main HTTP Error 404: Not Found
Traceback (most recent call last):
  File "main.py", line 10, in <module>
    data_ingestion.main()
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\pipeline\stage_01_data_ingestion.py", line 15, in main
    data_ingestion.download_file()
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\components\data_ingestion.py", line 17, in download_file
    filename, headers = request.urlretrieve(
  File "C:\Users\Sahil\anaconda3\envs\textS\lib\urllib\request.py", line 247, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
  File "C:\Users\Sahil\anaconda3\envs\textS\lib\urllib\request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "C:\Users\Sahil\anaconda3\envs\textS\lib\urllib\request.py", line 531, in open
    response = meth(req, response)
  File "C:\Users\Sahil\anaconda3\envs\textS\lib\urllib\request.py", line 640, in http_response
    response = self.parent.error(
  File "C:\Users\Sahil\anaconda3\envs\textS\lib\urllib\request.py", line 569, in error
    return self._call_chain(*args)
  File "C:\Users\Sahil\anaconda3\envs\textS\lib\urllib\request.py", line 502, in _call_chain
    result = func(*args)
  File "C:\Users\Sahil\anaconda3\envs\textS\lib\urllib\request.py", line 649, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
[2024-08-13 00:25:15,852]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-13 00:25:15,885]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:25:15,887]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:25:15,889]: INFO: common Created directory at: artifacts
[2024-08-13 00:25:15,890]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-13 00:25:20,102]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 2675:86A31:5D6D4:6768F:66BAE02A
Accept-Ranges: bytes
Date: Tue, 13 Aug 2024 04:25:17 GMT
Via: 1.1 varnish
X-Served-By: cache-bos4642-BOS
X-Cache: MISS
X-Cache-Hits: 0
X-Timer: S1723523117.078269,VS0,VE292
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: 54fbf1fbc1b0c1609ebba9dcee5b2fa72c7f1091
Expires: Tue, 13 Aug 2024 04:30:17 GMT
Source-Age: 0


[2024-08-13 00:25:20,374]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-13 00:25:20,374]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-13 00:25:20,374]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:25:20,374]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:25:20,389]: INFO: common Created directory at: artifacts
[2024-08-13 00:25:20,391]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-13 00:25:20,391]: ERROR: main 'DataIngestionConfig' object has no attribute 'ALL_REQUIRED_FILES'
Traceback (most recent call last):
  File "main.py", line 21, in <module>
    data_validation.main()
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\pipeline\stage_02_data_validation.py", line 15, in main
    data_validation.validate_all_files_exist()
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\components\data_validation.py", line 30, in validate_all_files_exist
    raise e
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\components\data_validation.py", line 18, in validate_all_files_exist
    if file not in self.config.ALL_REQUIRED_FILES:
AttributeError: 'DataIngestionConfig' object has no attribute 'ALL_REQUIRED_FILES'
[2024-08-13 00:27:36,457]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-13 00:27:36,461]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:27:36,461]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:27:36,461]: INFO: common Created directory at: artifacts
[2024-08-13 00:27:36,461]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-13 00:27:40,315]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 2675:86A31:5D6D4:6768F:66BAE02A
Accept-Ranges: bytes
Date: Tue, 13 Aug 2024 04:27:37 GMT
Via: 1.1 varnish
X-Served-By: cache-bos4678-BOS
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1723523258.660679,VS0,VE1
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: cb7caf71a7990e5de02bc321802f1b2497b62246
Expires: Tue, 13 Aug 2024 04:32:37 GMT
Source-Age: 140


[2024-08-13 00:27:40,523]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-13 00:27:40,523]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-13 00:27:40,523]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:27:40,523]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:27:40,523]: INFO: common Created directory at: artifacts
[2024-08-13 00:27:40,523]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-13 00:27:40,523]: ERROR: main 'DataIngestionConfig' object has no attribute 'ALL_REQUIRED_FILES'
Traceback (most recent call last):
  File "main.py", line 21, in <module>
    data_validation.main()
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\pipeline\stage_02_data_validation.py", line 15, in main
    data_validation.validate_all_files_exist()
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\components\data_validation.py", line 30, in validate_all_files_exist
    raise e
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\components\data_validation.py", line 18, in validate_all_files_exist
    if file not in self.config.ALL_REQUIRED_FILES:
AttributeError: 'DataIngestionConfig' object has no attribute 'ALL_REQUIRED_FILES'
[2024-08-13 00:29:41,078]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-13 00:29:41,093]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:29:41,093]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:29:41,093]: INFO: common Created directory at: artifacts
[2024-08-13 00:29:41,093]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-13 00:29:45,697]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 2675:86A31:5D6D4:6768F:66BAE02A
Accept-Ranges: bytes
Date: Tue, 13 Aug 2024 04:29:42 GMT
Via: 1.1 varnish
X-Served-By: cache-bos4664-BOS
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1723523382.192256,VS0,VE1
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: b32dc4d232df0de7afe0afd447bbddec0c96413b
Expires: Tue, 13 Aug 2024 04:34:42 GMT
Source-Age: 265


[2024-08-13 00:29:45,904]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-13 00:29:45,904]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-13 00:29:45,908]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:29:45,909]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:29:45,909]: INFO: common Created directory at: artifacts
[2024-08-13 00:29:45,909]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-13 00:29:45,909]: ERROR: main 'DataIngestionConfig' object has no attribute 'ALL_REQUIRED_FILES'
Traceback (most recent call last):
  File "main.py", line 21, in <module>
    data_validation.main()
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\pipeline\stage_02_data_validation.py", line 15, in main
    data_validation.validate_all_files_exist()
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\components\data_validation.py", line 30, in validate_all_files_exist
    raise e
  File "d:\unh materials\projects\text-summarizer\src\textSummarizer\components\data_validation.py", line 18, in validate_all_files_exist
    if file not in self.config.ALL_REQUIRED_FILES:
AttributeError: 'DataIngestionConfig' object has no attribute 'ALL_REQUIRED_FILES'
[2024-08-13 00:33:28,198]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-13 00:33:28,198]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:33:28,198]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:33:28,198]: INFO: common Created directory at: artifacts
[2024-08-13 00:33:28,198]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-13 00:33:33,243]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 2675:86A31:5D6D4:6768F:66BAE02A
Accept-Ranges: bytes
Date: Tue, 13 Aug 2024 04:33:29 GMT
Via: 1.1 varnish
X-Served-By: cache-bos4636-BOS
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1723523609.497869,VS0,VE121
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: e7cf8c383bda2251bcb5610f4b8566a8a2cd6ca7
Expires: Tue, 13 Aug 2024 04:38:29 GMT
Source-Age: 0


[2024-08-13 00:33:33,459]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-13 00:33:33,459]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-13 00:33:33,462]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:33:33,462]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:33:33,462]: INFO: common Created directory at: artifacts
[2024-08-13 00:33:33,467]: INFO: common Created directory at: artifacts/data_validation
[2024-08-13 00:33:33,467]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-13 00:52:49,211]: INFO: config PyTorch version 2.4.0 available.
[2024-08-13 00:58:39,956]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 00:58:39,971]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 00:58:39,972]: INFO: common Created directory at: artifacts
[2024-08-13 00:58:39,973]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-13 01:08:03,232]: INFO: config PyTorch version 2.4.0 available.
[2024-08-13 01:08:03,585]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-13 01:08:03,632]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 01:08:03,632]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 01:08:03,632]: INFO: common Created directory at: artifacts
[2024-08-13 01:08:03,632]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-13 01:08:07,645]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 2675:86A31:5D6D4:6768F:66BAE02A
Accept-Ranges: bytes
Date: Tue, 13 Aug 2024 05:08:05 GMT
Via: 1.1 varnish
X-Served-By: cache-bos4639-BOS
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1723525685.034057,VS0,VE52
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: ea1935a8513a57c1ca45d4dd0c7bde7d032e28bb
Expires: Tue, 13 Aug 2024 05:13:05 GMT
Source-Age: 0


[2024-08-13 01:08:07,808]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-13 01:08:07,808]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-13 01:08:07,808]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 01:08:07,808]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 01:08:07,808]: INFO: common Created directory at: artifacts
[2024-08-13 01:08:07,808]: INFO: common Created directory at: artifacts/data_validation
[2024-08-13 01:08:07,808]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-13 01:08:07,808]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-13 01:08:07,823]: INFO: common yaml file config\config.yaml loaded successfully.
[2024-08-13 01:08:07,824]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-13 01:08:07,825]: INFO: common Created directory at: artifacts
[2024-08-13 01:08:07,825]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-13 01:08:11,706]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 14:03:37,457]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 21:38:09,257]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 21:38:09,805]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 21:38:09,808]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 21:38:09,809]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 21:38:09,809]: INFO: common Created directory at: artifacts
[2024-08-14 21:38:09,809]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 21:38:10,369]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 71E2:1325B2:82B7F:90D84:66BD8631
Accept-Ranges: bytes
Date: Thu, 15 Aug 2024 04:38:10 GMT
Via: 1.1 varnish
X-Served-By: cache-lga21936-LGA
X-Cache: MISS
X-Cache-Hits: 0
X-Timer: S1723696690.123448,VS0,VE161
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: 6e121e2ebab452ee601f21670e6326f22bb3b2f1
Expires: Thu, 15 Aug 2024 04:43:10 GMT
Source-Age: 0


[2024-08-14 21:38:10,474]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 21:38:10,474]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 21:38:10,476]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 21:38:10,477]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 21:38:10,477]: INFO: common Created directory at: artifacts
[2024-08-14 21:38:10,477]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 21:38:10,477]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 21:38:10,477]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 21:38:10,479]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 21:38:10,480]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 21:38:10,480]: INFO: common Created directory at: artifacts
[2024-08-14 21:38:10,481]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 21:38:13,669]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 21:38:13,670]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 21:38:13,673]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 21:38:13,674]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 21:38:13,674]: INFO: common Created directory at: artifacts
[2024-08-14 21:38:13,674]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 21:39:27,814]: ERROR: main '>' not supported between instances of 'str' and 'int'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 19, in train
    trainer_args = TrainingArguments(
  File "<string>", line 131, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/training_args.py", line 1585, in __post_init__
    if self.save_strategy == IntervalStrategy.STEPS and self.save_steps > 1:
TypeError: '>' not supported between instances of 'str' and 'int'
[2024-08-14 21:43:34,075]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 21:43:34,621]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 21:43:34,624]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 21:43:34,625]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 21:43:34,625]: INFO: common Created directory at: artifacts
[2024-08-14 21:43:34,625]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 21:43:35,040]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 71E2:1325B2:82B7F:90D84:66BD8631
Accept-Ranges: bytes
Date: Thu, 15 Aug 2024 04:43:34 GMT
Via: 1.1 varnish
X-Served-By: cache-lga21940-LGA
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1723697015.890658,VS0,VE52
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: 94fbb195541b82395433c1ea2afb40c8fce1e33a
Expires: Thu, 15 Aug 2024 04:48:34 GMT
Source-Age: 0


[2024-08-14 21:43:35,145]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 21:43:35,145]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 21:43:35,148]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 21:43:35,149]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 21:43:35,149]: INFO: common Created directory at: artifacts
[2024-08-14 21:43:35,149]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 21:43:35,149]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 21:43:35,149]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 21:43:35,151]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 21:43:35,152]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 21:43:35,152]: INFO: common Created directory at: artifacts
[2024-08-14 21:43:35,153]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 21:43:37,851]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 21:43:37,852]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 21:43:37,854]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 21:43:37,855]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 21:43:37,855]: INFO: common Created directory at: artifacts
[2024-08-14 21:43:37,855]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:06:11,158]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:06:11,740]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:06:11,742]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:06:11,744]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:06:11,744]: INFO: common Created directory at: artifacts
[2024-08-14 22:06:11,744]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:06:12,170]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 71E2:1325B2:82B7F:90D84:66BD8631
Accept-Ranges: bytes
Date: Thu, 15 Aug 2024 05:06:12 GMT
Via: 1.1 varnish
X-Served-By: cache-lga21924-LGA
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1723698372.042539,VS0,VE44
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: 347c05101ddeaf85476af3bf2bb4d8eba37d3768
Expires: Thu, 15 Aug 2024 05:11:12 GMT
Source-Age: 0


[2024-08-14 22:06:12,276]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:06:12,276]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:06:12,278]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:06:12,279]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:06:12,279]: INFO: common Created directory at: artifacts
[2024-08-14 22:06:12,279]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:06:12,279]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:06:12,280]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:06:12,282]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:06:12,283]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:06:12,283]: INFO: common Created directory at: artifacts
[2024-08-14 22:06:12,283]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:06:15,069]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:06:15,069]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:06:15,071]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:06:15,072]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:06:15,072]: INFO: common Created directory at: artifacts
[2024-08-14 22:06:15,073]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:06:22,604]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-08-14 22:07:01,189]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:07:01,761]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:07:01,764]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:07:01,765]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:07:01,765]: INFO: common Created directory at: artifacts
[2024-08-14 22:07:01,765]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:07:01,765]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:07:01,875]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:07:01,875]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:07:01,877]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:07:01,878]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:07:01,878]: INFO: common Created directory at: artifacts
[2024-08-14 22:07:01,878]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:07:01,878]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:07:01,878]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:07:01,880]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:07:01,881]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:07:01,881]: INFO: common Created directory at: artifacts
[2024-08-14 22:07:01,881]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:07:02,875]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:07:02,875]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:07:02,878]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:07:02,879]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:07:02,879]: INFO: common Created directory at: artifacts
[2024-08-14 22:07:02,879]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:07:10,473]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 49.81 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.25 GiB is allocated by PyTorch, and 354.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 49.81 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.25 GiB is allocated by PyTorch, and 354.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-08-14 22:10:18,231]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:10:18,817]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:10:18,819]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:10:18,820]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:10:18,820]: INFO: common Created directory at: artifacts
[2024-08-14 22:10:18,821]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:10:19,205]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 71E2:1325B2:82B7F:90D84:66BD8631
Accept-Ranges: bytes
Date: Thu, 15 Aug 2024 05:10:19 GMT
Via: 1.1 varnish
X-Served-By: cache-lga21992-LGA
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1723698619.116232,VS0,VE1
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: 3484569b676362f267a26632fe51a9350ca882a3
Expires: Thu, 15 Aug 2024 05:15:19 GMT
Source-Age: 247


[2024-08-14 22:10:19,310]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:10:19,310]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:10:19,313]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:10:19,314]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:10:19,314]: INFO: common Created directory at: artifacts
[2024-08-14 22:10:19,314]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:10:19,314]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:10:19,314]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:10:19,316]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:10:19,317]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:10:19,317]: INFO: common Created directory at: artifacts
[2024-08-14 22:10:19,318]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:10:22,192]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:10:22,192]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:10:22,195]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:10:22,196]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:10:22,196]: INFO: common Created directory at: artifacts
[2024-08-14 22:10:22,196]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:10:29,798]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 49.81 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.25 GiB is allocated by PyTorch, and 354.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 59, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 49.81 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.25 GiB is allocated by PyTorch, and 354.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-08-14 22:15:12,893]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:15:13,492]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:15:13,494]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:15:13,495]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:15:13,496]: INFO: common Created directory at: artifacts
[2024-08-14 22:15:13,496]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:15:13,496]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:15:13,605]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:15:13,605]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:15:13,607]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:15:13,608]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:15:13,608]: INFO: common Created directory at: artifacts
[2024-08-14 22:15:13,608]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:15:13,609]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:15:13,609]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:15:13,611]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:15:13,612]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:15:13,612]: INFO: common Created directory at: artifacts
[2024-08-14 22:15:13,612]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:15:14,600]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:15:14,601]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:15:14,604]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:15:14,605]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:15:14,605]: INFO: common Created directory at: artifacts
[2024-08-14 22:15:14,605]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:15:19,994]: ERROR: main No columns in the dataset match the model's forward method signature. The following columns have been ignored: [dialogue, labels, attention_mask, input_ids, id, summary]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 62, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1977, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 902, in get_train_dataloader
    train_dataset = self._remove_unused_columns(train_dataset, description="training")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 830, in _remove_unused_columns
    raise ValueError(
ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [dialogue, labels, attention_mask, input_ids, id, summary]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
[2024-08-14 22:17:43,440]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:17:43,992]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:17:43,995]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:17:43,996]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:17:43,996]: INFO: common Created directory at: artifacts
[2024-08-14 22:17:43,996]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:17:43,996]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:17:44,105]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:17:44,105]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:17:44,107]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:17:44,108]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:17:44,108]: INFO: common Created directory at: artifacts
[2024-08-14 22:17:44,108]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:17:44,109]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:17:44,109]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:17:44,111]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:17:44,112]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:17:44,112]: INFO: common Created directory at: artifacts
[2024-08-14 22:17:44,112]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:17:45,085]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:17:45,085]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:17:45,088]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:17:45,089]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:17:45,089]: INFO: common Created directory at: artifacts
[2024-08-14 22:17:45,089]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:17:50,869]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-14 22:18:51,929]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:18:52,491]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:18:52,493]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:18:52,494]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:18:52,495]: INFO: common Created directory at: artifacts
[2024-08-14 22:18:52,495]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:18:52,495]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:18:52,604]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:18:52,604]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:18:52,606]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:18:52,607]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:18:52,607]: INFO: common Created directory at: artifacts
[2024-08-14 22:18:52,607]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:18:52,608]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:18:52,608]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:18:52,610]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:18:52,611]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:18:52,611]: INFO: common Created directory at: artifacts
[2024-08-14 22:18:52,611]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:18:53,484]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:18:53,484]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:18:53,486]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:18:53,487]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:18:53,487]: INFO: common Created directory at: artifacts
[2024-08-14 22:18:53,487]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:18:59,228]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-14 22:19:31,204]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:19:31,765]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:19:31,768]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:19:31,769]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:19:31,769]: INFO: common Created directory at: artifacts
[2024-08-14 22:19:31,769]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:19:31,769]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:19:31,881]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:19:31,881]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:19:31,883]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:19:31,884]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:19:31,884]: INFO: common Created directory at: artifacts
[2024-08-14 22:19:31,884]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:19:31,885]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:19:31,885]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:19:31,887]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:19:31,888]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:19:31,888]: INFO: common Created directory at: artifacts
[2024-08-14 22:19:31,888]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:19:32,742]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:19:32,743]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:19:32,745]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:19:32,746]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:19:32,746]: INFO: common Created directory at: artifacts
[2024-08-14 22:19:32,746]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:20:42,212]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:20:42,790]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:20:42,793]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:20:42,794]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:20:42,794]: INFO: common Created directory at: artifacts
[2024-08-14 22:20:42,794]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:20:42,794]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:20:42,904]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:20:42,904]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:20:42,906]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:20:42,907]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:20:42,907]: INFO: common Created directory at: artifacts
[2024-08-14 22:20:42,907]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:20:42,907]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:20:42,907]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:20:42,910]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:20:42,910]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:20:42,911]: INFO: common Created directory at: artifacts
[2024-08-14 22:20:42,911]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:20:43,786]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:20:43,786]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:20:43,788]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:20:43,789]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:20:43,789]: INFO: common Created directory at: artifacts
[2024-08-14 22:20:43,789]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:22:34,078]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:22:34,660]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:22:34,662]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:22:34,663]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:22:34,663]: INFO: common Created directory at: artifacts
[2024-08-14 22:22:34,664]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:22:35,134]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 71E2:1325B2:82B7F:90D84:66BD8631
Accept-Ranges: bytes
Date: Thu, 15 Aug 2024 05:22:35 GMT
Via: 1.1 varnish
X-Served-By: cache-lga21921-LGA
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1723699355.981975,VS0,VE68
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: d3423fed20f92c13b48e0da548a4a668f725ec4c
Expires: Thu, 15 Aug 2024 05:27:35 GMT
Source-Age: 0


[2024-08-14 22:22:35,240]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:22:35,240]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:22:35,242]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:22:35,243]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:22:35,243]: INFO: common Created directory at: artifacts
[2024-08-14 22:22:35,243]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:22:35,244]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:22:35,244]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:22:35,246]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:22:35,247]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:22:35,247]: INFO: common Created directory at: artifacts
[2024-08-14 22:22:35,247]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:22:38,055]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:22:38,055]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:22:38,058]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:22:38,059]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:22:38,059]: INFO: common Created directory at: artifacts
[2024-08-14 22:22:38,059]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:22:50,258]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 69.81 MiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 20.09 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 69.81 MiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 20.09 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-08-14 22:23:13,293]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:23:13,880]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:23:13,882]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:23:13,884]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:23:13,884]: INFO: common Created directory at: artifacts
[2024-08-14 22:23:13,884]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:23:13,884]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:23:13,996]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:23:13,996]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:23:13,998]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:23:13,999]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:23:13,999]: INFO: common Created directory at: artifacts
[2024-08-14 22:23:13,999]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:23:14,000]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:23:14,000]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:23:14,002]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:23:14,003]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:23:14,003]: INFO: common Created directory at: artifacts
[2024-08-14 22:23:14,003]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:23:15,008]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:23:15,008]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:23:15,012]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:23:15,013]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:23:15,014]: INFO: common Created directory at: artifacts
[2024-08-14 22:23:15,014]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:23:27,326]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 71.81 MiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 20.09 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 71.81 MiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 20.09 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-08-14 22:25:18,614]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:25:19,200]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:25:19,203]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:25:19,204]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:25:19,204]: INFO: common Created directory at: artifacts
[2024-08-14 22:25:19,204]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:25:19,205]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:25:19,315]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:25:19,315]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:25:19,317]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:25:19,318]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:25:19,319]: INFO: common Created directory at: artifacts
[2024-08-14 22:25:19,319]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:25:19,319]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:25:19,319]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:25:19,321]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:25:19,322]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:25:19,322]: INFO: common Created directory at: artifacts
[2024-08-14 22:25:19,322]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:25:20,499]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:25:20,500]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:25:20,502]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:25:20,504]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:25:20,504]: INFO: common Created directory at: artifacts
[2024-08-14 22:25:20,504]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:25:32,765]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 71.81 MiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 20.09 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 71.81 MiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 20.09 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-08-14 22:25:52,431]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:25:53,016]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:25:53,018]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:25:53,019]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:25:53,019]: INFO: common Created directory at: artifacts
[2024-08-14 22:25:53,020]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:25:53,020]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:25:53,130]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:25:53,130]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:25:53,132]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:25:53,133]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:25:53,133]: INFO: common Created directory at: artifacts
[2024-08-14 22:25:53,133]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:25:53,133]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:25:53,133]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:25:53,135]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:25:53,136]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:25:53,136]: INFO: common Created directory at: artifacts
[2024-08-14 22:25:53,136]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:25:53,986]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:25:53,986]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:25:53,989]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:25:53,990]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:25:53,990]: INFO: common Created directory at: artifacts
[2024-08-14 22:25:53,990]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:26:06,093]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 71.81 MiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 20.09 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 425, in forward
    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 218, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 71.81 MiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 20.09 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-08-14 22:27:03,811]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:27:04,399]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:27:04,401]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:27:04,402]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:27:04,402]: INFO: common Created directory at: artifacts
[2024-08-14 22:27:04,402]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:27:04,403]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:27:04,512]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:27:04,512]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:27:04,514]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:27:04,515]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:27:04,515]: INFO: common Created directory at: artifacts
[2024-08-14 22:27:04,515]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:27:04,515]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:27:04,516]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:27:04,518]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:27:04,519]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:27:04,519]: INFO: common Created directory at: artifacts
[2024-08-14 22:27:04,519]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:27:05,410]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:27:05,411]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:27:05,413]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:27:05,414]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:27:05,414]: INFO: common Created directory at: artifacts
[2024-08-14 22:27:05,414]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:27:10,748]: ERROR: main No columns in the dataset match the model's forward method signature. The following columns have been ignored: [dialogue, attention_mask, id, input_ids, summary, labels]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1977, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 902, in get_train_dataloader
    train_dataset = self._remove_unused_columns(train_dataset, description="training")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 830, in _remove_unused_columns
    raise ValueError(
ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [dialogue, attention_mask, id, input_ids, summary, labels]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
[2024-08-14 22:27:22,105]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:27:22,656]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:27:22,659]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:27:22,660]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:27:22,660]: INFO: common Created directory at: artifacts
[2024-08-14 22:27:22,660]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:27:22,660]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:27:22,770]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:27:22,770]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:27:22,772]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:27:22,773]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:27:22,773]: INFO: common Created directory at: artifacts
[2024-08-14 22:27:22,773]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:27:22,773]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:27:22,773]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:27:22,775]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:27:22,776]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:27:22,776]: INFO: common Created directory at: artifacts
[2024-08-14 22:27:22,777]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:27:23,631]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:27:23,631]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:27:23,633]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:27:23,634]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:27:23,634]: INFO: common Created directory at: artifacts
[2024-08-14 22:27:23,634]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:27:29,382]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-14 22:31:22,548]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:31:23,097]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:31:23,099]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:31:23,101]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:31:23,101]: INFO: common Created directory at: artifacts
[2024-08-14 22:31:23,101]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:31:23,101]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:31:23,210]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:31:23,210]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:31:23,212]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:31:23,213]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:31:23,213]: INFO: common Created directory at: artifacts
[2024-08-14 22:31:23,213]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:31:23,214]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:31:23,214]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:31:23,216]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:31:23,217]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:31:23,217]: INFO: common Created directory at: artifacts
[2024-08-14 22:31:23,217]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:31:24,094]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:31:24,095]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:31:24,097]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:31:24,098]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:31:24,098]: INFO: common Created directory at: artifacts
[2024-08-14 22:31:24,098]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:31:29,800]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-14 22:32:06,802]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:32:07,352]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:32:07,355]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:32:07,356]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:32:07,356]: INFO: common Created directory at: artifacts
[2024-08-14 22:32:07,356]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:32:07,356]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:32:07,466]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:32:07,466]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:32:07,468]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:32:07,469]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:32:07,469]: INFO: common Created directory at: artifacts
[2024-08-14 22:32:07,469]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:32:07,469]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:32:07,469]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:32:07,471]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:32:07,472]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:32:07,472]: INFO: common Created directory at: artifacts
[2024-08-14 22:32:07,472]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:32:08,326]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:32:08,326]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:32:08,329]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:32:08,330]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:32:08,330]: INFO: common Created directory at: artifacts
[2024-08-14 22:32:08,330]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:32:14,069]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-14 22:32:52,745]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:32:53,295]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:32:53,297]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:32:53,298]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:32:53,299]: INFO: common Created directory at: artifacts
[2024-08-14 22:32:53,299]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:32:53,299]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:32:53,408]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:32:53,408]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:32:53,410]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:32:53,411]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:32:53,411]: INFO: common Created directory at: artifacts
[2024-08-14 22:32:53,411]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:32:53,412]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:32:53,412]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:32:53,414]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:32:53,415]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:32:53,415]: INFO: common Created directory at: artifacts
[2024-08-14 22:32:53,415]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:32:54,292]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:32:54,293]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:32:54,295]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:32:54,296]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:32:54,296]: INFO: common Created directory at: artifacts
[2024-08-14 22:32:54,296]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:33:41,492]: ERROR: main CUDA out of memory. Tried to allocate 2.39 GiB. GPU 0 has a total capacity of 21.96 GiB of which 185.81 MiB is free. Including non-PyTorch memory, this process has 21.77 GiB memory in use. Of the allocated memory 20.88 GiB is allocated by PyTorch, and 584.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 187, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 204, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 109, in gather
    res = gather_map(outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/utils/generic.py", line 390, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 94, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.39 GiB. GPU 0 has a total capacity of 21.96 GiB of which 185.81 MiB is free. Including non-PyTorch memory, this process has 21.77 GiB memory in use. Of the allocated memory 20.88 GiB is allocated by PyTorch, and 584.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-08-14 22:34:29,948]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:34:30,530]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:34:30,533]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:34:30,534]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:34:30,534]: INFO: common Created directory at: artifacts
[2024-08-14 22:34:30,534]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:34:30,534]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:34:30,643]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:34:30,644]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:34:30,646]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:34:30,647]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:34:30,647]: INFO: common Created directory at: artifacts
[2024-08-14 22:34:30,647]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:34:30,647]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:34:30,647]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:34:30,649]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:34:30,650]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:34:30,650]: INFO: common Created directory at: artifacts
[2024-08-14 22:34:30,650]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:34:31,518]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:34:31,518]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:34:31,520]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:34:31,521]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:34:31,521]: INFO: common Created directory at: artifacts
[2024-08-14 22:34:31,521]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-14 22:36:05,462]: INFO: config PyTorch version 2.4.0 available.
[2024-08-14 22:36:06,653]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-08-14 22:36:06,656]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:36:06,657]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:36:06,657]: INFO: common Created directory at: artifacts
[2024-08-14 22:36:06,657]: INFO: common Created directory at: artifacts/data_ingestion
[2024-08-14 22:36:06,657]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-08-14 22:36:06,767]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:36:06,767]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-08-14 22:36:06,769]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:36:06,770]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:36:06,770]: INFO: common Created directory at: artifacts
[2024-08-14 22:36:06,770]: INFO: common Created directory at: artifacts/data_validation
[2024-08-14 22:36:06,770]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:36:06,770]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-08-14 22:36:06,773]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:36:06,773]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:36:06,774]: INFO: common Created directory at: artifacts
[2024-08-14 22:36:06,774]: INFO: common Created directory at: artifacts/data_transformation
[2024-08-14 22:36:07,644]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-08-14 22:36:07,644]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-14 22:36:07,647]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-14 22:36:07,648]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-14 22:36:07,648]: INFO: common Created directory at: artifacts
[2024-08-14 22:36:07,648]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-15 00:53:20,534]: INFO: main >>>>>>>>>> stage Model Training Stage completed <<<<<<<<<

x=============x
[2024-08-15 00:53:20,535]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 00:53:20,537]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 00:53:20,538]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 00:53:20,538]: INFO: common Created directory at: artifacts
[2024-08-15 00:53:20,538]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 00:53:39,513]: ERROR: main The repository for rouge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/rouge.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
Traceback (most recent call last):
  File "main.py", line 59, in <module>
    model_evaluation.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_05_model_evaluation.py", line 15, in main
    model_evaluation.evaluate()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_evaluation.py", line 51, in evaluate
    rouge_metric = load_metric('rouge')
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/utils/deprecation_utils.py", line 46, in wrapper
    return deprecated_function(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 2121, in load_metric
    metric_module = metric_module_factory(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/utils/deprecation_utils.py", line 46, in wrapper
    return deprecated_function(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 2039, in metric_module_factory
    raise e1 from None
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 2026, in metric_module_factory
    return GithubMetricModuleFactory(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 803, in get_module
    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 134, in resolve_trust_remote_code
    raise ValueError(
ValueError: The repository for rouge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/rouge.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
[2024-08-15 21:56:51,143]: INFO: config PyTorch version 2.4.0 available.
[2024-08-15 21:56:51,693]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 21:56:51,696]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 21:56:51,697]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 21:56:51,697]: INFO: common Created directory at: artifacts
[2024-08-15 21:56:51,697]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 21:57:10,844]: ERROR: main The repository for rouge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/rouge.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
Traceback (most recent call last):
  File "main.py", line 59, in <module>
    model_evaluation.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_05_model_evaluation.py", line 15, in main
    model_evaluation.evaluate()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_evaluation.py", line 51, in evaluate
    rouge_metric = load_metric('rouge')
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/utils/deprecation_utils.py", line 46, in wrapper
    return deprecated_function(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 2121, in load_metric
    metric_module = metric_module_factory(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/utils/deprecation_utils.py", line 46, in wrapper
    return deprecated_function(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 2039, in metric_module_factory
    raise e1 from None
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 2026, in metric_module_factory
    return GithubMetricModuleFactory(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 803, in get_module
    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/datasets/load.py", line 134, in resolve_trust_remote_code
    raise ValueError(
ValueError: The repository for rouge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/rouge.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
[2024-08-15 22:00:06,466]: INFO: config PyTorch version 2.4.0 available.
[2024-08-15 22:00:07,132]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 22:00:07,135]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 22:00:07,136]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 22:00:07,136]: INFO: common Created directory at: artifacts
[2024-08-15 22:00:07,136]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 22:00:13,658]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:15,780]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:17,900]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:20,021]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:22,143]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:24,272]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:26,410]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:28,330]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:30,457]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:32,346]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:34,472]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:36,289]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:37,847]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:39,119]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:41,246]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:43,382]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:44,713]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:46,842]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:48,977]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:49,954]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:52,090]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:54,230]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:56,368]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:57,625]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:00:59,759]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:01,901]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:02,851]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:04,985]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:06,747]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:08,887]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:09,621]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:11,757]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:12,749]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:14,177]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:16,219]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:17,380]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:19,451]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:21,031]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:22,141]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:24,278]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:26,420]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:27,818]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:29,957]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:30,826]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:32,522]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:33,952]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:36,092]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:38,029]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:38,949]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:40,960]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:43,101]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:45,245]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:46,494]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:47,695]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:48,939]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:50,292]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:51,212]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:52,383]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:54,523]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:56,200]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:57,035]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:01:59,174]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:00,024]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:01,451]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:03,597]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:04,714]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:05,461]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:07,603]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:09,749]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:11,304]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:12,717]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:13,726]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:15,094]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:16,673]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:18,817]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:19,652]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:20,927]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:23,069]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:25,213]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:27,358]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:28,412]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:30,553]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:32,398]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:34,540]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:35,969]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:38,110]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:40,255]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:42,054]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:44,195]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:45,565]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:46,677]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:48,818]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:50,961]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:51,741]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:53,643]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:54,787]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:56,929]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:57,793]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:02:59,144]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:00,482]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:02,627]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:03,423]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:04,173]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:04,950]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:05,885]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:07,947]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:08,872]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:10,031]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:10,810]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:12,952]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:15,099]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:17,248]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:19,001]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:21,145]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:23,292]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:25,389]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:27,532]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:28,981]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:30,169]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:32,313]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:33,950]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:36,092]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:37,029]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:39,171]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:39,994]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:42,136]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:43,295]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:45,438]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:46,478]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:47,473]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:48,718]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:50,863]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:53,011]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:55,153]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:56,904]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:03:59,049]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:01,194]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:02,787]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:03,753]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:05,029]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:05,910]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:08,051]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:10,073]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:10,839]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:11,774]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:12,916]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:14,621]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:16,765]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:18,579]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:20,344]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:21,166]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:23,309]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:25,450]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:27,597]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:29,408]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:31,552]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:32,613]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:34,752]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:35,724]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:36,880]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:38,231]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:39,903]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:41,075]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:41,881]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:44,022]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:46,167]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:48,313]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:49,683]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:51,827]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:53,970]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:56,116]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:58,260]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:04:59,833]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:00,899]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:02,238]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:03,099]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:05,159]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:06,067]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:06,843]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:08,452]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:09,597]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:11,738]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:13,884]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:15,312]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:16,604]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:17,450]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:19,509]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:21,652]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:23,327]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:25,000]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:27,144]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:28,577]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:30,719]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:32,862]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:35,008]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:37,151]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:39,137]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:40,493]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:41,458]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:43,113]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:44,419]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:46,560]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:48,704]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:50,848]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:52,989]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:55,139]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:56,629]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:05:58,769]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:00,916]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:03,065]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:04,922]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:06,004]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:08,147]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:09,407]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:10,638]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:12,779]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:14,924]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:17,071]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:18,746]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:20,666]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:22,809]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:23,775]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:25,916]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:28,061]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:30,208]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:31,188]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:32,152]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:34,292]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:35,725]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:37,868]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:38,968]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:39,902]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:41,131]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:41,893]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:42,653]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:44,795]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:46,937]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:49,086]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:51,233]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:53,379]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:54,491]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:55,425]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:56,837]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:06:58,978]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:00,337]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:01,904]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:03,487]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:05,632]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:07,172]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:08,478]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:10,622]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:12,406]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:13,742]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:14,723]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:16,380]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:18,087]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:20,230]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:22,378]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:23,766]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:25,907]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:28,052]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:28,862]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:30,655]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:32,800]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:34,034]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:35,011]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:37,153]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:39,299]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:40,093]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:42,236]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:43,274]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:45,352]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:46,464]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:47,253]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:48,289]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:49,111]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:51,256]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:53,398]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:54,880]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:56,877]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:07:58,465]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:00,608]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:02,750]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:04,820]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:06,963]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:08,638]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:10,781]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:12,926]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:13,838]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:15,979]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:17,836]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:19,978]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:22,126]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:22,895]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:23,612]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:24,359]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:26,498]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:28,643]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:29,801]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:31,942]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:33,711]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:35,852]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:37,997]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:40,147]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:42,293]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:43,206]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:44,848]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:45,886]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:48,025]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:50,170]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:52,318]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:53,932]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:54,750]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:56,891]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:58,171]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:08:59,597]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:01,314]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:03,456]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:05,597]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:07,363]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:08,744]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:09,479]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:11,119]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:12,217]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:14,357]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:15,809]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:17,948]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:19,635]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:20,426]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:22,565]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:24,709]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:25,809]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:26,801]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:28,612]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:30,755]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:32,492]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:34,632]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:36,777]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:37,908]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:40,049]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:42,192]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:43,086]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:45,226]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:47,371]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:49,513]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:50,974]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:53,114]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:53,877]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:54,722]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:55,512]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:57,654]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:09:59,802]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:01,369]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:03,512]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:05,434]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:06,680]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:08,822]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:10,962]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:12,792]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:14,936]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:16,039]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:18,181]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:20,009]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:22,149]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:23,161]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:24,754]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:26,107]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:28,057]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:30,038]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:30,960]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:33,099]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:35,248]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:37,394]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:38,643]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:40,783]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:42,933]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:45,076]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:46,117]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:47,544]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:48,377]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:50,517]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:52,192]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:53,040]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:54,635]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:56,414]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:57,497]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:10:59,168]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:00,119]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:02,260]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:04,412]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:06,561]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:08,708]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:09,732]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:10,754]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:12,893]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:14,537]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:16,680]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:18,710]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:20,858]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:23,001]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:25,146]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:26,109]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:28,250]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:29,012]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:31,153]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:33,298]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:34,591]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:36,733]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:37,466]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:39,181]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:40,400]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:41,599]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:43,711]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:45,852]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:47,073]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:49,214]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:50,287]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:51,717]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:52,946]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:55,084]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:57,228]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:11:59,372]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:01,141]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:03,281]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:05,172]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:07,312]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:09,454]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:11,599]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:13,738]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:15,880]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:18,026]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:20,176]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:22,098]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:23,939]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:26,079]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:28,222]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:30,361]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:31,652]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:33,461]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:35,602]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:36,671]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:37,811]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:39,951]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:40,685]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:42,019]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:44,045]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:46,191]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:47,579]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:49,719]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:50,628]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:52,735]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:53,600]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:54,591]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:55,895]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:57,199]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:12:59,336]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:01,476]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:03,615]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:05,757]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:07,906]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:09,108]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:11,245]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:13,386]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:14,676]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:16,455]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:17,780]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:18,743]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:20,881]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:22,111]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:24,251]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:26,390]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:28,388]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:30,526]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:31,577]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:33,138]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:34,732]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:36,872]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:39,013]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:41,156]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:42,345]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:44,485]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:45,875]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:46,868]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:49,007]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:50,725]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:51,659]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:52,889]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:54,650]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:55,497]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:57,633]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:13:58,698]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:00,836]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:01,950]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:03,001]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:05,139]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:07,280]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:09,426]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:11,564]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:13,703]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:15,012]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:16,487]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:18,626]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:20,767]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:22,047]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:24,184]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:24,916]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:27,055]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:29,202]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:31,341]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:32,436]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:33,698]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:35,169]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:37,179]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:38,127]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:39,584]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:41,391]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:42,167]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:43,652]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:45,790]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:47,615]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:48,955]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:50,034]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:51,656]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:53,113]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:55,093]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:57,104]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:14:59,242]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:00,640]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:01,370]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:03,396]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:05,131]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:07,274]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:08,935]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:09,810]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:11,946]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:13,133]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:13,922]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:15,529]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:16,682]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:18,199]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:20,340]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:22,484]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:24,480]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:26,414]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:27,410]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:28,577]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:29,629]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:30,535]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:31,672]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:33,035]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:34,446]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:35,406]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:37,542]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:38,278]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:40,417]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:42,559]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:44,051]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:46,189]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:48,076]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:49,381]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:50,313]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:51,186]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:52,691]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:54,830]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:56,889]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:58,501]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:15:59,407]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:01,513]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:02,629]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:03,446]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:04,525]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:06,503]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:08,128]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:10,266]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:12,123]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:14,266]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:16,406]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:18,061]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:20,200]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:21,050]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:22,520]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:23,280]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:24,199]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:26,339]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:28,483]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:30,622]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:32,462]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:34,605]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:36,748]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:38,891]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:41,060]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:43,201]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:44,066]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:46,202]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:48,140]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:49,236]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:50,496]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:51,343]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:52,739]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:53,557]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:55,694]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:57,837]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:16:59,614]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:00,346]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:01,559]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:02,668]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:04,805]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:06,949]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:07,869]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:08,931]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:10,174]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:12,312]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:14,453]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:15,358]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:16,631]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:17,724]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:19,862]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:21,999]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:23,923]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:26,062]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:27,859]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:30,001]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:32,143]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:34,285]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:36,423]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:38,567]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:40,709]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:42,852]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:44,992]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:46,270]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:47,102]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:48,166]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:49,085]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:51,223]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:53,317]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:55,460]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:57,445]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:17:59,589]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:01,728]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:03,628]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:05,767]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:07,120]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:09,256]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:11,394]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:12,699]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:14,837]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:16,984]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:19,126]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:21,264]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:23,408]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:24,140]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:25,717]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:27,858]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:29,004]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:30,718]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:32,858]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:34,093]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:34,938]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:36,455]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:37,405]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:38,322]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:40,459]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:41,206]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:43,344]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:45,481]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:47,627]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:48,665]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:50,802]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:52,941]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:53,760]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:55,896]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:58,036]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:18:59,280]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:01,418]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:03,559]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:05,697]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:07,838]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:08,631]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:10,768]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:12,907]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:15,050]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:15,860]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:17,999]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:20,139]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:21,061]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:22,883]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:23,976]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:26,114]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:27,866]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:29,005]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:31,142]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:32,192]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:34,330]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:35,880]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:36,813]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:38,951]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:39,906]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:42,042]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:44,182]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:45,220]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:47,358]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:49,277]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:51,415]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:53,554]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:54,473]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:56,607]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:57,853]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:19:59,991]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:00,998]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:01,958]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:03,669]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:05,477]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:07,615]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:09,593]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:11,731]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:12,464]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:13,676]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:15,009]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:16,678]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:17,817]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:19,241]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:20,606]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:22,743]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:23,604]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:24,786]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:25,821]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:27,960]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:29,896]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:32,035]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:34,176]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:36,138]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:37,577]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:38,729]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:40,866]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:43,005]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:45,145]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:46,363]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:47,893]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:50,030]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:51,703]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:53,617]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:55,755]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:20:57,897]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:00,035]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:01,449]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:03,586]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:04,876]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:07,010]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:08,444]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:09,961]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:12,099]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:14,097]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:16,234]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:18,371]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:19,526]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:20,544]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:21,362]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:22,395]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:23,834]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:25,974]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:27,176]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:28,924]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:30,260]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:31,684]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:33,822]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:35,965]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:37,033]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:38,113]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:40,249]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:42,391]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:44,532]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:45,705]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:46,915]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:49,053]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:49,788]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:51,748]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:53,022]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:54,218]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:55,092]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:57,228]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:58,373]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:21:59,408]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:01,545]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:03,680]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:05,818]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:06,739]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:08,513]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:10,652]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:11,607]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:13,743]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:15,644]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:16,934]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:17,937]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:19,649]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:21,787]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:23,927]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:24,804]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:26,273]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:27,133]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:28,711]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:30,849]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:31,767]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:33,903]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:36,040]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:37,091]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:37,836]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:38,566]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:40,703]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:41,696]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:42,526]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:44,659]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:46,795]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:48,935]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:51,074]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:53,209]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:53,932]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:54,995]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:57,130]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:59,269]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:22:59,372]: ERROR: main 'numpy.float64' object has no attribute 'mid'
Traceback (most recent call last):
  File "main.py", line 59, in <module>
    model_evaluation.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_05_model_evaluation.py", line 15, in main
    model_evaluation.evaluate()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_evaluation.py", line 58, in evaluate
    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_evaluation.py", line 58, in <genexpr>
    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)
AttributeError: 'numpy.float64' object has no attribute 'mid'
[2024-08-15 22:29:54,615]: INFO: config PyTorch version 2.4.0 available.
[2024-08-15 22:29:55,282]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 22:29:55,284]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 22:29:55,285]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 22:29:55,285]: INFO: common Created directory at: artifacts
[2024-08-15 22:29:55,285]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 22:30:01,802]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:30:03,931]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:30:06,056]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:30:08,189]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:30:10,314]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:30:10,425]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
[2024-08-15 22:34:32,923]: INFO: config PyTorch version 2.4.0 available.
[2024-08-15 22:34:33,588]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 22:34:33,591]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 22:34:33,592]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 22:34:33,592]: INFO: common Created directory at: artifacts
[2024-08-15 22:34:33,592]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 22:34:40,201]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:34:42,325]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:34:44,441]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:34:46,564]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:34:48,682]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:34:48,791]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
[2024-08-15 22:35:00,517]: INFO: config PyTorch version 2.4.0 available.
[2024-08-15 22:35:01,182]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 22:35:01,185]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 22:35:01,186]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 22:35:01,186]: INFO: common Created directory at: artifacts
[2024-08-15 22:35:01,186]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 22:35:08,687]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:11,765]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:14,847]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:14,971]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
[2024-08-15 22:35:24,776]: INFO: config PyTorch version 2.4.0 available.
[2024-08-15 22:35:25,442]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 22:35:25,444]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 22:35:25,446]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 22:35:25,446]: INFO: common Created directory at: artifacts
[2024-08-15 22:35:25,446]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 22:35:32,972]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:36,046]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:39,128]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:42,204]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:45,268]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:48,327]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:50,549]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:53,630]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:56,669]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:35:59,682]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:02,768]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:05,800]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:08,887]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:11,905]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:14,969]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:17,977]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:19,997]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:22,876]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:25,833]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:28,852]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:31,890]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:34,893]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:37,322]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:40,393]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:43,235]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:46,327]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:48,110]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:50,036]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:51,701]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:54,750]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:57,764]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:36:59,773]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:02,794]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:05,802]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:08,849]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:10,851]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:13,104]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:16,112]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:19,144]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:22,237]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:25,261]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:28,332]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:31,378]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:34,448]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:37,485]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:40,508]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:43,507]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:46,196]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:49,205]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:51,139]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:54,142]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:55,257]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:58,159]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:37:59,803]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:02,808]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:05,900]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:08,969]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:12,057]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:15,098]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:18,130]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:21,193]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:24,208]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:27,216]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:30,241]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:31,728]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:34,757]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:37,852]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:40,914]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:43,970]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:45,764]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:48,773]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:51,603]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:53,225]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:56,281]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:38:58,886]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:01,895]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:04,983]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:08,049]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:11,084]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:12,738]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:15,116]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:16,771]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:19,863]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:22,903]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:25,995]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:29,086]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:31,303]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:33,190]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:36,082]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:38,346]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:41,370]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:44,409]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:46,235]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:49,319]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:51,719]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:54,757]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:39:57,848]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:00,945]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:03,766]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:06,103]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:09,136]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:12,228]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:15,321]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:18,379]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:21,478]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:24,109]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:27,139]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:30,170]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:33,257]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:36,012]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:39,026]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:42,118]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:45,136]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:48,158]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:51,204]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:52,766]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:54,500]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:40:57,506]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:00,594]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:03,691]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:05,279]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:08,319]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:10,553]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:13,604]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:15,796]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:18,858]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:20,753]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:23,192]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:26,285]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:29,339]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:32,339]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:35,409]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:37,163]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:40,253]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:43,271]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:46,201]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:47,779]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:49,257]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:52,348]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:55,204]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:41:58,257]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:01,342]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:04,403]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:07,497]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:10,522]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:13,592]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:16,600]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:17,680]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:20,763]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:23,798]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:26,868]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:29,959]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:32,977]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:35,298]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:38,384]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:41,443]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:44,454]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:46,489]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:49,547]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:52,606]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:54,545]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:56,862]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:42:59,903]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:02,960]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:05,965]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:08,985]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:11,539]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:14,595]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:17,685]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:20,719]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:23,726]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:26,818]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:29,869]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:32,875]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:34,088]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:37,178]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:40,239]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:42,961]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:46,052]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:49,121]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:52,152]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:55,226]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:43:57,481]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:00,252]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:03,034]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:06,126]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:09,167]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:12,262]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:15,289]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:17,302]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:20,354]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:22,598]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:25,106]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:27,453]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:30,539]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:33,637]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:35,115]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:38,159]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:41,244]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:44,337]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:47,354]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:50,353]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:53,440]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:56,480]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:44:58,893]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:00,631]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:03,719]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:06,751]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:08,781]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:11,810]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:14,903]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:17,976]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:21,049]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:24,140]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:27,229]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:30,324]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:33,094]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:36,184]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:39,219]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:42,286]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:43,919]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:46,911]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:49,797]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:52,839]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:55,852]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:45:58,815]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:00,670]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:03,705]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:06,795]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:09,891]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:12,929]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:15,965]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:18,492]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:21,512]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:24,545]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:27,630]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:30,649]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:32,936]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:36,025]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:39,054]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:42,096]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:45,123]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:47,541]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:50,047]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:53,053]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:56,071]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:46:57,661]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:00,749]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:03,848]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:06,876]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:09,926]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:12,958]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:15,961]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:19,051]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:22,078]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:24,175]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:27,004]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:29,585]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:31,682]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:34,743]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:36,648]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:38,967]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:41,866]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:44,904]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:47,754]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:50,816]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:53,153]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:56,174]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:47:58,443]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:00,595]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:03,688]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:06,571]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:08,246]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:09,746]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:11,684]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:13,674]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:16,675]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:19,770]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:22,826]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:25,499]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:26,838]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:29,882]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:32,827]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:35,798]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:37,374]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:40,169]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:43,223]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:46,295]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:49,354]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:52,359]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:54,431]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:48:57,444]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:00,539]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:03,610]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:06,708]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:09,807]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:12,828]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:15,566]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:17,345]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:19,310]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:22,402]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:24,896]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:26,630]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:29,719]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:31,253]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:34,281]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:37,295]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:39,113]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:42,197]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:45,280]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:48,350]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:51,443]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:54,538]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:49:57,633]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:00,672]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:02,189]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:05,204]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:08,293]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:11,378]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:14,456]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:17,491]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:20,583]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:23,620]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:26,711]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:29,804]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:32,029]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:35,050]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:38,115]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:39,861]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:41,997]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:45,007]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:48,013]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:51,104]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:54,128]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:50:57,129]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:00,217]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:03,253]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:06,346]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:09,349]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:12,440]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:15,446]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:18,535]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:21,109]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:24,134]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:26,608]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:29,628]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:32,678]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:35,697]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:38,715]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:41,733]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:44,802]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:47,897]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:50,921]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:53,956]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:55,399]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:51:57,991]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:01,067]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:04,068]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:05,975]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:08,332]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:10,372]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:13,381]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:15,071]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:18,146]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:21,245]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:24,050]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:27,074]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:30,163]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:32,351]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:35,407]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:38,483]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:41,579]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:44,627]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:47,665]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:49,838]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:52,917]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:56,012]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:57,661]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:52:59,140]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:02,186]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:04,667]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:06,703]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:09,792]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:11,351]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:14,439]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:17,469]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:20,502]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:23,263]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:25,082]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:28,090]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:29,728]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:32,815]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:35,822]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:38,893]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:41,916]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:44,612]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:47,034]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:50,125]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:52,206]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:54,428]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:53:57,433]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:00,522]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:02,017]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:05,013]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:06,428]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:09,513]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:12,608]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:15,607]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:18,637]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:20,785]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 22:54:20,892]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
[2024-08-15 23:01:06,211]: INFO: config PyTorch version 2.4.0 available.
[2024-08-15 23:01:06,877]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 23:01:06,880]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 23:01:06,881]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 23:01:06,881]: INFO: common Created directory at: artifacts
[2024-08-15 23:01:06,881]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 23:01:14,347]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:01:17,415]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:01:19,547]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:01:19,656]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
[2024-08-15 23:02:05,788]: INFO: config PyTorch version 2.4.0 available.
[2024-08-15 23:02:06,453]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 23:02:06,456]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 23:02:06,457]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 23:02:06,457]: INFO: common Created directory at: artifacts
[2024-08-15 23:02:06,457]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 23:02:13,941]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:02:17,011]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:02:20,082]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:02:20,206]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
[2024-08-15 23:02:50,305]: INFO: config PyTorch version 2.4.0 available.
[2024-08-15 23:02:50,971]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-15 23:02:50,973]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-15 23:02:50,974]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-15 23:02:50,975]: INFO: common Created directory at: artifacts
[2024-08-15 23:02:50,975]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-15 23:02:58,475]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:01,541]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:04,618]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:07,685]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:10,742]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:13,796]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:16,019]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:19,096]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:22,129]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:25,134]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:28,214]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:31,242]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:34,325]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:37,343]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:40,399]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:43,411]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:45,428]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:48,312]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:51,268]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:54,287]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:03:57,322]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:00,327]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:02,752]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:05,818]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:08,654]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:11,735]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:13,521]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:15,448]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:17,111]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:20,157]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:23,166]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:25,175]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:28,193]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:31,200]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:34,245]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:36,247]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:38,492]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:41,493]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:44,522]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:47,613]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:50,634]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:53,700]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:56,747]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:04:59,810]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:02,844]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:05,867]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:08,865]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:11,552]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:14,553]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:16,485]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:19,485]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:20,603]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:23,500]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:25,144]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:28,148]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:31,241]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:34,303]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:37,394]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:40,434]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:43,465]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:46,524]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:49,538]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:52,543]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:55,564]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:05:57,053]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:00,078]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:03,170]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:06,230]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:09,278]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:11,073]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:14,078]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:16,905]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:18,526]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:21,580]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:24,184]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:27,193]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:30,282]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:33,346]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:36,379]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:38,030]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:40,413]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:42,070]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:45,162]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:48,202]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:51,294]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:54,388]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:56,605]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:06:58,492]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:01,381]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:03,646]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:06,672]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:09,710]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:11,537]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:14,622]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:17,022]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:20,061]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:23,152]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:26,245]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:29,066]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:31,401]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:34,432]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:37,521]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:40,620]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:43,678]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:46,777]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:49,409]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:52,437]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:55,466]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:07:58,554]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:01,302]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:04,316]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:07,405]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:10,422]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:13,440]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:16,485]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:18,049]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:19,783]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:22,788]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:25,873]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:28,969]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:30,560]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:33,599]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:35,834]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:38,885]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:41,077]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:44,137]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:46,028]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:48,471]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:51,561]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:54,611]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:08:57,611]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:00,674]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:02,423]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:05,514]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:08,529]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:11,456]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:13,029]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:14,501]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:17,589]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:20,437]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:23,493]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:26,579]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:29,638]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:32,727]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:35,749]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:38,822]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:41,833]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:42,913]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:45,999]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:49,028]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:52,093]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:55,181]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:09:58,197]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:00,514]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:03,599]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:06,658]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:09,664]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:11,698]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:14,758]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:17,822]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:19,765]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:22,082]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:25,122]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:28,176]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:31,185]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:34,202]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:36,755]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:39,812]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:42,901]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:45,930]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:48,936]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:52,022]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:55,071]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:58,074]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:10:59,288]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:02,375]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:05,436]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:08,150]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:11,235]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:14,297]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:17,325]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:20,390]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:22,641]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:25,410]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:28,189]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:31,272]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:34,307]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:37,399]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:40,419]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:42,425]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:45,473]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:47,717]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:50,225]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:52,570]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:55,653]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:11:58,750]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:00,221]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:03,262]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:06,342]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:09,431]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:12,441]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:15,442]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:18,527]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:21,562]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:23,973]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:25,709]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:28,791]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:31,819]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:33,842]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:36,869]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:39,950]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:43,015]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:46,084]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:49,171]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:52,256]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:55,345]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:12:58,110]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:01,196]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:04,223]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:07,291]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:08,921]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:11,908]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:14,791]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:17,825]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:20,833]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:23,790]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:25,641]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:28,673]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:31,753]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:34,842]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:37,873]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:40,903]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:43,425]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:46,441]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:49,465]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:52,535]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:55,545]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:13:57,823]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:00,907]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:03,932]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:06,965]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:09,985]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:12,399]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:14,898]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:17,898]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:20,912]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:22,499]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:25,586]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:28,676]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:31,700]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:34,743]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:37,769]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:40,767]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:43,847]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:46,869]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:48,961]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:51,784]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:54,359]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:56,448]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:14:59,502]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:01,398]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:03,710]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:06,603]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:09,639]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:12,482]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:15,541]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:17,874]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:20,889]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:23,148]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:25,295]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:28,375]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:31,250]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:32,921]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:34,422]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:36,360]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:38,351]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:41,341]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:44,432]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:47,483]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:50,154]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:51,493]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:54,536]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:15:57,470]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:00,437]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:02,011]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:04,804]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:07,851]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:10,915]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:13,964]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:16,967]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:19,033]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:22,040]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:25,129]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:28,192]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:31,284]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:34,374]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:37,388]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:40,114]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:41,888]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:43,849]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:46,927]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:49,428]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:51,159]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:54,242]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:55,774]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:16:58,800]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:01,804]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:03,615]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:06,696]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:09,772]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:12,835]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:15,922]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:19,011]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:22,098]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:25,132]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:26,648]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:29,655]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:32,741]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:35,819]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:38,890]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:41,919]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:45,001]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:48,033]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:51,121]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:54,205]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:56,423]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:17:59,435]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:02,493]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:04,238]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:06,369]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:09,378]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:12,378]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:15,459]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:18,482]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:21,483]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:24,570]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:27,596]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:30,684]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:33,683]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:36,766]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:39,768]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:42,851]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:45,421]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:48,439]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:50,909]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:53,919]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:56,961]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:18:59,973]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:02,987]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:06,001]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:09,065]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:12,153]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:15,166]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:18,195]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:19,636]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:22,222]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:25,296]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:28,294]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:30,195]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:32,550]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:34,583]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:37,585]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:39,271]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:42,336]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:45,424]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:48,217]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:51,238]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:54,321]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:56,511]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:19:59,563]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:02,630]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:05,718]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:08,759]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:11,789]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:13,958]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:17,030]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:20,114]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:21,761]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:23,231]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:26,267]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:28,745]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:30,780]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:33,866]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:35,418]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:38,504]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:41,527]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:44,553]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:47,306]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:49,123]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:52,129]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:53,764]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:56,841]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:20:59,843]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:02,899]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:05,918]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:08,609]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:11,031]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:14,115]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:16,191]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:18,409]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:21,411]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:24,493]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:25,986]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:28,985]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:30,402]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:33,485]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:36,573]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:39,568]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:42,585]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:44,727]: INFO: rouge_scorer Using default tokenizer.
[2024-08-15 23:21:44,834]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
[2024-08-16 07:05:33,556]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 07:05:34,222]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-08-16 07:05:34,224]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 07:05:34,225]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 07:05:34,226]: INFO: common Created directory at: artifacts
[2024-08-16 07:05:34,226]: INFO: common Created directory at: artifacts/model_evaluation
[2024-08-16 07:05:41,712]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:05:44,779]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:05:47,850]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:05:50,915]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:05:53,970]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:05:57,021]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:05:59,238]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:02,311]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:05,343]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:08,344]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:11,422]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:14,450]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:17,534]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:20,547]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:23,601]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:26,604]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:28,618]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:31,496]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:34,452]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:37,471]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:40,505]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:43,510]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:45,934]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:49,003]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:51,839]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:54,928]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:56,712]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:06:58,636]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:00,300]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:03,353]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:06,368]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:08,376]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:11,397]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:14,406]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:17,454]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:19,454]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:21,706]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:24,706]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:27,736]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:30,822]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:33,850]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:36,918]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:39,962]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:43,031]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:46,069]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:49,093]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:52,093]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:54,781]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:57,789]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:07:59,724]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:02,727]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:03,845]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:06,744]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:08,389]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:11,394]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:14,486]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:17,556]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:20,643]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:23,684]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:26,714]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:29,777]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:32,791]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:35,801]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:38,825]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:40,314]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:43,343]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:46,435]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:49,496]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:52,544]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:54,337]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:08:57,343]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:00,168]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:01,790]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:04,844]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:07,448]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:10,457]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:13,546]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:16,612]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:19,645]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:21,294]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:23,677]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:25,333]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:28,425]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:31,465]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:34,558]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:37,648]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:39,866]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:41,755]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:44,646]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:46,910]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:49,935]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:52,971]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:54,796]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:09:57,874]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:00,272]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:03,311]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:06,404]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:09,497]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:12,313]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:14,652]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:17,685]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:20,774]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:23,866]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:26,923]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:30,020]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:32,647]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:35,678]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:38,709]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:41,799]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:44,554]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:47,567]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:50,658]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:53,679]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:56,697]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:10:59,743]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:01,310]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:03,044]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:06,050]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:09,136]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:12,232]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:13,823]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:16,863]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:19,098]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:22,148]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:24,340]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:27,402]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:29,296]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:31,736]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:34,826]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:37,880]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:40,881]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:43,945]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:45,698]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:48,784]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:51,798]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:54,726]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:56,303]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:11:57,774]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:00,860]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:03,711]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:06,764]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:09,847]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:12,914]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:16,009]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:19,033]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:22,105]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:25,116]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:26,199]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:29,283]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:32,320]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:35,384]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:38,480]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:41,499]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:43,817]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:46,903]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:49,962]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:52,976]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:55,009]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:12:58,068]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:01,128]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:03,069]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:05,385]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:08,426]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:11,482]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:14,491]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:17,511]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:20,067]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:23,123]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:26,211]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:29,244]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:32,249]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:35,341]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:38,387]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:41,396]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:42,612]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:45,700]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:48,761]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:51,485]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:54,575]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:13:57,642]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:00,671]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:03,740]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:05,998]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:08,765]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:11,547]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:14,634]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:17,671]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:20,765]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:23,790]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:25,804]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:28,854]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:31,099]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:33,609]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:35,958]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:39,043]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:42,139]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:43,616]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:46,659]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:49,740]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:52,834]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:55,847]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:14:58,845]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:01,932]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:04,971]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:07,380]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:09,122]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:12,208]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:15,237]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:17,263]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:20,290]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:23,378]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:26,445]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:29,515]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:32,602]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:35,686]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:38,778]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:41,547]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:44,638]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:47,666]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:50,734]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:52,365]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:55,354]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:15:58,236]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:01,272]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:04,282]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:07,243]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:09,095]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:12,125]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:15,210]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:18,304]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:21,337]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:24,367]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:26,893]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:29,912]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:32,944]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:36,021]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:39,038]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:41,321]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:44,405]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:47,431]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:50,466]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:53,489]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:55,904]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:16:58,409]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:01,414]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:04,430]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:06,020]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:09,107]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:12,199]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:15,228]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:18,272]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:21,305]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:24,305]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:27,387]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:30,413]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:32,513]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:35,339]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:37,915]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:40,006]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:43,063]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:44,962]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:47,274]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:50,170]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:53,205]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:56,057]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:17:59,114]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:01,453]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:04,470]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:06,731]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:08,879]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:11,968]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:14,841]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:16,512]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:18,010]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:19,947]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:21,935]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:24,928]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:28,014]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:31,065]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:33,738]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:35,076]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:38,119]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:41,055]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:44,021]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:45,598]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:48,392]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:51,445]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:54,516]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:18:57,569]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:00,571]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:02,639]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:05,649]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:08,737]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:11,808]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:14,899]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:17,990]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:21,008]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:23,737]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:25,520]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:27,486]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:30,568]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:33,069]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:34,799]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:37,883]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:39,418]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:42,445]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:45,448]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:47,268]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:50,349]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:53,425]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:56,492]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:19:59,579]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:02,673]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:05,763]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:08,795]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:10,312]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:13,319]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:16,407]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:19,487]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:22,557]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:25,591]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:28,675]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:31,712]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:34,804]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:37,890]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:40,113]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:43,132]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:46,195]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:47,941]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:50,073]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:53,082]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:56,082]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:20:59,165]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:02,186]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:05,189]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:08,273]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:11,303]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:14,390]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:17,391]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:20,475]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:23,482]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:26,568]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:29,136]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:32,160]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:34,631]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:37,640]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:40,683]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:43,698]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:46,714]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:49,726]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:52,793]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:55,881]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:21:58,901]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:01,927]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:03,374]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:05,963]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:09,039]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:12,039]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:13,945]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:16,306]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:18,343]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:21,347]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:23,034]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:26,100]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:29,190]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:31,990]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:35,010]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:38,093]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:40,277]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:43,328]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:46,404]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:49,498]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:52,543]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:55,574]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:22:57,744]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:00,814]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:03,900]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:05,547]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:07,024]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:10,064]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:12,543]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:14,581]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:17,669]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:19,224]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:22,308]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:25,330]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:28,358]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:31,118]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:32,940]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:35,947]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:37,581]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:40,663]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:43,666]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:46,727]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:49,747]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:52,438]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:54,858]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:23:57,941]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:00,023]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:02,238]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:05,239]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:08,320]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:09,817]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:12,810]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:14,226]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:17,309]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:20,396]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:23,390]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:26,410]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:28,553]: INFO: rouge_scorer Using default tokenizer.
[2024-08-16 07:24:28,660]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
[2024-08-16 17:30:24,158]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 17:30:24,818]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 17:30:24,821]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 17:30:24,822]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 17:30:24,822]: INFO: common Created directory at: artifacts
[2024-08-16 17:30:24,822]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 17:30:32,524]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-08-16 17:48:26,091]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 17:48:26,793]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 17:48:26,796]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 17:48:26,797]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 17:48:26,797]: INFO: common Created directory at: artifacts
[2024-08-16 17:48:26,797]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 17:48:32,005]: ERROR: main __init__() got an unexpected keyword argument 'auto_find_batch_size_size'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 38, in train
    trainer_args = TrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'auto_find_batch_size_size'
[2024-08-16 17:51:22,177]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 17:51:22,842]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 17:51:22,845]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 17:51:22,846]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 17:51:22,846]: INFO: common Created directory at: artifacts
[2024-08-16 17:51:22,846]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 17:51:28,477]: ERROR: main No columns in the dataset match the model's forward method signature. The following columns have been ignored: [attention_mask, summary, dialogue, input_ids, labels, id]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 65, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/utils/memory.py", line 146, in decorator
    return function(batch_size, *args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1977, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 902, in get_train_dataloader
    train_dataset = self._remove_unused_columns(train_dataset, description="training")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 830, in _remove_unused_columns
    raise ValueError(
ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [attention_mask, summary, dialogue, input_ids, labels, id]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
[2024-08-16 17:52:11,976]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 17:52:12,643]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 17:52:12,646]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 17:52:12,647]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 17:52:12,647]: INFO: common Created directory at: artifacts
[2024-08-16 17:52:12,647]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 17:53:33,117]: ERROR: main No executable batch size found, reached zero.
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 65, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/utils/memory.py", line 144, in decorator
    raise RuntimeError("No executable batch size found, reached zero.")
RuntimeError: No executable batch size found, reached zero.
[2024-08-16 17:55:14,304]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 17:55:14,992]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 17:55:14,995]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 17:55:14,996]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 17:55:14,996]: INFO: common Created directory at: artifacts
[2024-08-16 17:55:14,996]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 17:55:20,797]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-16 17:55:36,441]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 17:55:37,108]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 17:55:37,111]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 17:55:37,112]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 17:55:37,112]: INFO: common Created directory at: artifacts
[2024-08-16 17:55:37,112]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 17:55:44,738]: ERROR: main CUDA out of memory. Tried to allocate 4.32 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 19.89 GiB memory in use. Of the allocated memory 19.41 GiB is allocated by PyTorch, and 175.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 187, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 204, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 109, in gather
    res = gather_map(outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/utils/generic.py", line 390, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 94, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.32 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 19.89 GiB memory in use. Of the allocated memory 19.41 GiB is allocated by PyTorch, and 175.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-08-16 17:56:17,092]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 17:56:17,969]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 17:56:17,971]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 17:56:17,973]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 17:56:17,973]: INFO: common Created directory at: artifacts
[2024-08-16 17:56:17,973]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 17:56:32,282]: ERROR: main CUDA out of memory. Tried to allocate 4.32 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 19.89 GiB memory in use. Of the allocated memory 19.41 GiB is allocated by PyTorch, and 175.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 187, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 204, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 109, in gather
    res = gather_map(outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/utils/generic.py", line 390, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 94, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.32 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 19.89 GiB memory in use. Of the allocated memory 19.41 GiB is allocated by PyTorch, and 175.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-08-16 17:57:26,958]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 17:57:27,645]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 17:57:27,648]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 17:57:27,649]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 17:57:27,649]: INFO: common Created directory at: artifacts
[2024-08-16 17:57:27,649]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 18:10:19,345]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 18:10:35,826]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 18:10:47,629]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 18:10:49,094]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 18:10:49,097]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 18:10:49,098]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 18:10:49,098]: INFO: common Created directory at: artifacts
[2024-08-16 18:10:49,098]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 18:10:55,995]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-16 18:23:50,865]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 18:23:51,526]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 18:23:51,528]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 18:23:51,529]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 18:23:51,530]: INFO: common Created directory at: artifacts
[2024-08-16 18:23:51,530]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 18:23:57,340]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-16 18:24:34,074]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 18:24:34,740]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 18:24:34,742]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 18:24:34,743]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 18:24:34,743]: INFO: common Created directory at: artifacts
[2024-08-16 18:24:34,743]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 18:24:40,541]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-16 18:27:46,424]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 18:27:47,089]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 18:27:47,091]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 18:27:47,092]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 18:27:47,092]: INFO: common Created directory at: artifacts
[2024-08-16 18:27:47,092]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 18:27:52,890]: ERROR: main Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 762, in convert_to_tensors
    tensor = as_tensor(value)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 724, in as_tensor
    return torch.tensor(value)
ValueError: too many dimensions 'str'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 598, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3560, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 227, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 778, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
[2024-08-16 18:28:11,015]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 18:28:11,681]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 18:28:11,683]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 18:28:11,684]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 18:28:11,685]: INFO: common Created directory at: artifacts
[2024-08-16 18:28:11,685]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 18:28:45,577]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 18:28:46,270]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 18:28:46,273]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 18:28:46,274]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 18:28:46,274]: INFO: common Created directory at: artifacts
[2024-08-16 18:28:46,274]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 18:28:54,066]: ERROR: main CUDA out of memory. Tried to allocate 4.32 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 19.89 GiB memory in use. Of the allocated memory 19.41 GiB is allocated by PyTorch, and 175.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 187, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 204, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 109, in gather
    res = gather_map(outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/utils/generic.py", line 390, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 94, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.32 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 19.89 GiB memory in use. Of the allocated memory 19.41 GiB is allocated by PyTorch, and 175.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-08-16 18:30:18,673]: INFO: config PyTorch version 2.4.0 available.
[2024-08-16 18:30:19,370]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-08-16 18:30:19,373]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-08-16 18:30:19,374]: INFO: common yaml file params.yaml loaded successfully.
[2024-08-16 18:30:19,374]: INFO: common Created directory at: artifacts
[2024-08-16 18:30:19,374]: INFO: common Created directory at: artifacts/model_trainer
[2024-08-16 18:30:33,671]: ERROR: main CUDA out of memory. Tried to allocate 2.09 GiB. GPU 0 has a total capacity of 21.96 GiB of which 467.81 MiB is free. Including non-PyTorch memory, this process has 21.49 GiB memory in use. Of the allocated memory 20.15 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 187, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 204, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 109, in gather
    res = gather_map(outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/utils/generic.py", line 390, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 94, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.09 GiB. GPU 0 has a total capacity of 21.96 GiB of which 467.81 MiB is free. Including non-PyTorch memory, this process has 21.49 GiB memory in use. Of the allocated memory 20.15 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-01 23:06:39,937]: INFO: config PyTorch version 2.4.0 available.
[2024-09-01 23:06:40,583]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-01 23:06:40,586]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-01 23:06:40,587]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-01 23:06:40,587]: INFO: common Created directory at: artifacts
[2024-09-01 23:06:40,587]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-01 23:06:54,880]: ERROR: main CUDA out of memory. Tried to allocate 2.09 GiB. GPU 0 has a total capacity of 21.96 GiB of which 467.81 MiB is free. Including non-PyTorch memory, this process has 21.49 GiB memory in use. Of the allocated memory 20.15 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 187, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 204, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 109, in gather
    res = gather_map(outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/utils/generic.py", line 390, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 94, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.09 GiB. GPU 0 has a total capacity of 21.96 GiB of which 467.81 MiB is free. Including non-PyTorch memory, this process has 21.49 GiB memory in use. Of the allocated memory 20.15 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-01 23:07:38,949]: INFO: config PyTorch version 2.4.0 available.
[2024-09-01 23:07:39,617]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-01 23:07:39,620]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-01 23:07:39,621]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-01 23:07:39,621]: INFO: common Created directory at: artifacts
[2024-09-01 23:07:39,621]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-01 23:07:47,218]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-01 23:11:17,872]: INFO: config PyTorch version 2.4.0 available.
[2024-09-01 23:11:18,541]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-01 23:11:18,543]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-01 23:11:18,544]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-01 23:11:18,544]: INFO: common Created directory at: artifacts
[2024-09-01 23:11:18,544]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-01 23:11:24,374]: ERROR: main Expected a 'cuda' device type for generator but found 'cpu'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 672, in _next_data
    index = self._next_index()  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 620, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 288, in __iter__
    for idx in self.sampler:
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 92, in __iter__
    yield from super().__iter__()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 168, in __iter__
    yield from torch.randperm(n, generator=generator).tolist()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/_device.py", line 79, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'
[2024-09-01 23:13:30,581]: INFO: config PyTorch version 2.4.0 available.
[2024-09-01 23:13:31,220]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-01 23:13:31,223]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-01 23:13:31,224]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-01 23:13:31,224]: INFO: common Created directory at: artifacts
[2024-09-01 23:13:31,224]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-01 23:13:37,200]: ERROR: main Expected a 'cuda' device type for generator but found 'cpu'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2246, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 672, in _next_data
    index = self._next_index()  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 620, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 288, in __iter__
    for idx in self.sampler:
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/accelerate/data_loader.py", line 92, in __iter__
    yield from super().__iter__()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/data/sampler.py", line 168, in __iter__
    yield from torch.randperm(n, generator=generator).tolist()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/utils/_device.py", line 79, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'
[2024-09-01 23:16:46,683]: INFO: config PyTorch version 2.4.0 available.
[2024-09-01 23:16:47,338]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-01 23:16:47,340]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-01 23:16:47,341]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-01 23:16:47,341]: INFO: common Created directory at: artifacts
[2024-09-01 23:16:47,341]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-01 23:16:54,915]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 09:50:48,117]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 09:50:48,780]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 09:50:48,783]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 09:50:48,784]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 09:50:48,784]: INFO: common Created directory at: artifacts
[2024-09-05 09:50:48,784]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 09:50:56,533]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 67, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 09:56:29,978]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 09:56:30,664]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 09:56:30,666]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 09:56:30,667]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 09:56:30,667]: INFO: common Created directory at: artifacts
[2024-09-05 09:56:30,667]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 09:56:38,193]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 71, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 09:59:04,705]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 09:59:05,365]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 09:59:05,368]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 09:59:05,369]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 09:59:05,369]: INFO: common Created directory at: artifacts
[2024-09-05 09:59:05,369]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 09:59:59,220]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 09:59:59,911]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 09:59:59,913]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 09:59:59,914]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 09:59:59,914]: INFO: common Created directory at: artifacts
[2024-09-05 09:59:59,914]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:00:14,176]: ERROR: main CUDA out of memory. Tried to allocate 2.09 GiB. GPU 0 has a total capacity of 21.96 GiB of which 467.81 MiB is free. Including non-PyTorch memory, this process has 21.49 GiB memory in use. Of the allocated memory 20.15 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 71, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 187, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 204, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 109, in gather
    res = gather_map(outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/utils/generic.py", line 390, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 94, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.09 GiB. GPU 0 has a total capacity of 21.96 GiB of which 467.81 MiB is free. Including non-PyTorch memory, this process has 21.49 GiB memory in use. Of the allocated memory 20.15 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 10:05:20,841]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:05:21,540]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:05:21,543]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:05:21,544]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:05:21,544]: INFO: common Created directory at: artifacts
[2024-09-05 10:05:21,544]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:05:29,316]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 73, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 10:06:02,804]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:06:03,505]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:06:03,507]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:06:03,508]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:06:03,508]: INFO: common Created directory at: artifacts
[2024-09-05 10:06:03,508]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:06:11,238]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 73, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 10:18:21,251]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:18:21,950]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:18:21,953]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:18:21,954]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:18:21,954]: INFO: common Created directory at: artifacts
[2024-09-05 10:18:21,954]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:18:29,707]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 73, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 10:19:54,892]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:19:55,591]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-09-05 10:19:55,593]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:19:55,594]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:19:55,594]: INFO: common Created directory at: artifacts
[2024-09-05 10:19:55,594]: INFO: common Created directory at: artifacts/data_transformation
[2024-09-05 10:19:56,446]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-09-05 10:19:56,446]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:19:56,449]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:19:56,450]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:19:56,450]: INFO: common Created directory at: artifacts
[2024-09-05 10:19:56,450]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:20:04,197]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 73, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1196, in forward
    encoder_outputs = self.encoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 785, in forward
    layer_outputs = encoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 318, in forward
    hidden_states = self.activation_fn(self.fc1(hidden_states))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 279.81 MiB is free. Including non-PyTorch memory, this process has 21.68 GiB memory in use. Of the allocated memory 21.06 GiB is allocated by PyTorch, and 313.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 10:25:32,032]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:25:32,735]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-09-05 10:25:32,737]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:25:32,738]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:25:32,739]: INFO: common Created directory at: artifacts
[2024-09-05 10:25:32,739]: INFO: common Created directory at: artifacts/data_ingestion
[2024-09-05 10:25:33,398]: INFO: data_ingestion artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 24F2:1F0E36:40DE6B:46FAE1:66D9E98C
Accept-Ranges: bytes
Date: Thu, 05 Sep 2024 17:25:33 GMT
Via: 1.1 varnish
X-Served-By: cache-lga21957-LGA
X-Cache: MISS
X-Cache-Hits: 0
X-Timer: S1725557133.126119,VS0,VE183
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: 9c8d0f03f4a5a3e9e428e5a15e5e219c7df8ac90
Expires: Thu, 05 Sep 2024 17:30:33 GMT
Source-Age: 0


[2024-09-05 10:25:33,504]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-09-05 10:25:33,504]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-09-05 10:25:33,506]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:25:33,507]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:25:33,507]: INFO: common Created directory at: artifacts
[2024-09-05 10:25:33,507]: INFO: common Created directory at: artifacts/data_validation
[2024-09-05 10:25:33,508]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-09-05 10:25:33,508]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-09-05 10:25:33,510]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:25:33,511]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:25:33,511]: INFO: common Created directory at: artifacts
[2024-09-05 10:25:33,511]: INFO: common Created directory at: artifacts/data_transformation
[2024-09-05 10:25:36,135]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-09-05 10:25:36,136]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:25:36,138]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:25:36,139]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:25:36,139]: INFO: common Created directory at: artifacts
[2024-09-05 10:25:36,139]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:25:44,161]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 443, in forward
    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 41.81 MiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 53.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 73, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 443, in forward
    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 41.81 MiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 53.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 10:32:59,590]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:33:00,291]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-09-05 10:33:00,294]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:33:00,295]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:33:00,295]: INFO: common Created directory at: artifacts
[2024-09-05 10:33:00,295]: INFO: common Created directory at: artifacts/data_ingestion
[2024-09-05 10:33:00,295]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-09-05 10:33:00,405]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-09-05 10:33:00,406]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-09-05 10:33:00,408]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:33:00,409]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:33:00,409]: INFO: common Created directory at: artifacts
[2024-09-05 10:33:00,409]: INFO: common Created directory at: artifacts/data_validation
[2024-09-05 10:33:00,409]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-09-05 10:33:00,409]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-09-05 10:33:00,412]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:33:00,413]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:33:00,413]: INFO: common Created directory at: artifacts
[2024-09-05 10:33:00,413]: INFO: common Created directory at: artifacts/data_transformation
[2024-09-05 10:33:01,538]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-09-05 10:33:01,539]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:33:01,541]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:33:01,543]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:33:01,543]: INFO: common Created directory at: artifacts
[2024-09-05 10:33:01,543]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:33:09,383]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 443, in forward
    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 41.81 MiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 53.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 73, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 443, in forward
    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 41.81 MiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 53.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 10:33:16,227]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:33:16,904]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:33:16,907]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:33:16,908]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:33:16,908]: INFO: common Created directory at: artifacts
[2024-09-05 10:33:16,908]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:33:24,843]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 443, in forward
    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 41.81 MiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 53.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 73, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 443, in forward
    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 41.81 MiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 53.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 10:42:42,647]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:42:43,338]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:42:43,340]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:42:43,341]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:42:43,341]: INFO: common Created directory at: artifacts
[2024-09-05 10:42:43,342]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:42:48,621]: ERROR: main 'PegasusTokenizerFast' object has no attribute 'to'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 27, in train
    tokenizer.to(device)
AttributeError: 'PegasusTokenizerFast' object has no attribute 'to'
[2024-09-05 10:43:25,083]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:43:25,751]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:43:25,754]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:43:25,755]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:43:25,755]: INFO: common Created directory at: artifacts
[2024-09-05 10:43:25,755]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:43:30,999]: ERROR: main 'DataCollatorForSeq2Seq' object has no attribute 'to'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 28, in train
    seq2seq_data_collator.to(device)
AttributeError: 'DataCollatorForSeq2Seq' object has no attribute 'to'
[2024-09-05 10:43:49,862]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 10:43:50,529]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 10:43:50,532]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 10:43:50,533]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 10:43:50,533]: INFO: common Created directory at: artifacts
[2024-09-05 10:43:50,533]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 10:43:58,328]: ERROR: main Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 443, in forward
    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 41.81 MiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 53.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 73, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 201, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 108, in parallel_apply
    output.reraise()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in _worker
    output = module(*input, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
    outputs = self.model(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1214, in forward
    decoder_outputs = self.decoder(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1046, in forward
    layer_outputs = decoder_layer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 443, in forward
    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 41.81 MiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 53.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-05 21:35:05,354]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:35:05,972]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:35:05,975]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:35:05,976]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:35:05,976]: INFO: common Created directory at: artifacts
[2024-09-05 21:35:05,976]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:35:11,485]: ERROR: main CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 26, in train
    model_pegasus.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 21:36:02,252]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:36:02,252]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:36:02,894]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:36:02,897]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:36:02,898]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:36:02,898]: INFO: common Created directory at: artifacts
[2024-09-05 21:36:02,898]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:36:02,900]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:36:02,903]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:36:02,904]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:36:02,904]: INFO: common Created directory at: artifacts
[2024-09-05 21:36:02,904]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:36:08,002]: ERROR: main CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 15.56 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 738.00 MiB memory in use. Process 1000898 has 770.00 MiB memory in use. Of the allocated memory 540.18 MiB is allocated by PyTorch, and 13.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 26, in train
    model_pegasus.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 15.56 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 738.00 MiB memory in use. Process 1000898 has 770.00 MiB memory in use. Of the allocated memory 540.18 MiB is allocated by PyTorch, and 13.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 21:36:08,004]: ERROR: main CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 15.56 MiB is free. Process 977795 has 20.45 GiB memory in use. Process 1000897 has 738.00 MiB memory in use. Including non-PyTorch memory, this process has 770.00 MiB memory in use. Of the allocated memory 584.21 MiB is allocated by PyTorch, and 1.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 26, in train
    model_pegasus.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 15.56 MiB is free. Process 977795 has 20.45 GiB memory in use. Process 1000897 has 738.00 MiB memory in use. Including non-PyTorch memory, this process has 770.00 MiB memory in use. Of the allocated memory 584.21 MiB is allocated by PyTorch, and 1.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 21:46:36,690]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:46:37,309]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:46:37,311]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:46:37,312]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:46:37,312]: INFO: common Created directory at: artifacts
[2024-09-05 21:46:37,312]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:46:42,366]: ERROR: main CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 26, in train
    model_pegasus.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 21:53:06,062]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:53:06,063]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:53:06,068]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:53:06,327]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:53:06,736]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:53:06,738]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:53:06,739]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:53:06,739]: INFO: common Created directory at: artifacts
[2024-09-05 21:53:06,739]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:53:06,742]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:53:06,744]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:53:06,745]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:53:06,746]: INFO: common Created directory at: artifacts
[2024-09-05 21:53:06,746]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:53:06,755]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:53:06,757]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:53:06,758]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:53:06,758]: INFO: common Created directory at: artifacts
[2024-09-05 21:53:06,758]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:53:06,972]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:53:06,974]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:53:06,975]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:53:06,975]: INFO: common Created directory at: artifacts
[2024-09-05 21:53:06,975]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:53:11,563]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 57, in train
    training_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 21:53:11,596]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 57, in train
    training_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 21:53:11,609]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 57, in train
    training_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 21:53:11,770]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 57, in train
    training_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 21:53:33,139]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:53:33,759]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:53:33,761]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:53:33,762]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:53:33,762]: INFO: common Created directory at: artifacts
[2024-09-05 21:53:33,763]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:53:38,464]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 57, in train
    training_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 21:55:31,274]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:55:31,893]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:55:31,895]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:55:31,896]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:55:31,896]: INFO: common Created directory at: artifacts
[2024-09-05 21:55:31,896]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:55:37,109]: ERROR: main CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 99, in train
    trainer = Trainer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 545, in __init__
    self._move_model_to_device(model, args.device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 792, in _move_model_to_device
    model = model.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 21:55:53,051]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:55:53,056]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:55:53,056]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:55:53,056]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 21:55:53,722]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:55:53,725]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:55:53,726]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:55:53,726]: INFO: common Created directory at: artifacts
[2024-09-05 21:55:53,726]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:55:53,748]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:55:53,750]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:55:53,751]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:55:53,751]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:55:53,751]: INFO: common Created directory at: artifacts
[2024-09-05 21:55:53,751]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:55:53,752]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 21:55:53,753]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:55:53,754]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:55:53,754]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 21:55:53,754]: INFO: common Created directory at: artifacts
[2024-09-05 21:55:53,754]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:55:53,755]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 21:55:53,755]: INFO: common Created directory at: artifacts
[2024-09-05 21:55:53,755]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 21:55:59,208]: ERROR: main CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 99, in train
    trainer = Trainer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 545, in __init__
    self._move_model_to_device(model, args.device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 792, in _move_model_to_device
    model = model.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 21:55:59,285]: ERROR: main trying to initialize the default process group twice!
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 110, in train
    torch.distributed.init_process_group(backend="nccl")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 93, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1302, in init_process_group
    raise ValueError("trying to initialize the default process group twice!")
ValueError: trying to initialize the default process group twice!
[2024-09-05 21:55:59,347]: ERROR: main trying to initialize the default process group twice!
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 110, in train
    torch.distributed.init_process_group(backend="nccl")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 93, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1302, in init_process_group
    raise ValueError("trying to initialize the default process group twice!")
ValueError: trying to initialize the default process group twice!
[2024-09-05 21:55:59,403]: ERROR: main trying to initialize the default process group twice!
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 110, in train
    torch.distributed.init_process_group(backend="nccl")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 93, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1302, in init_process_group
    raise ValueError("trying to initialize the default process group twice!")
ValueError: trying to initialize the default process group twice!
[2024-09-05 22:02:41,085]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:02:41,091]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:02:41,093]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:02:41,095]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:02:41,781]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:02:41,783]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:02:41,784]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:02:41,784]: INFO: common Created directory at: artifacts
[2024-09-05 22:02:41,784]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:02:41,793]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:02:41,795]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:02:41,796]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:02:41,797]: INFO: common Created directory at: artifacts
[2024-09-05 22:02:41,797]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:02:41,807]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:02:41,809]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:02:41,811]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:02:41,811]: INFO: common Created directory at: artifacts
[2024-09-05 22:02:41,811]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:02:41,813]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:02:41,815]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:02:41,816]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:02:41,816]: INFO: common Created directory at: artifacts
[2024-09-05 22:02:41,816]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:02:46,514]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 22:02:46,602]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 22:02:46,618]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 22:02:46,654]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 22:03:50,829]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:03:50,830]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:03:50,839]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:03:50,854]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:03:51,505]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:03:51,507]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:03:51,508]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:03:51,508]: INFO: common Created directory at: artifacts
[2024-09-05 22:03:51,508]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:03:51,519]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:03:51,521]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:03:51,522]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:03:51,522]: INFO: common Created directory at: artifacts
[2024-09-05 22:03:51,523]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:03:51,541]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:03:51,544]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:03:51,545]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:03:51,545]: INFO: common Created directory at: artifacts
[2024-09-05 22:03:51,545]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:03:51,551]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:03:51,553]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:03:51,554]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:03:51,554]: INFO: common Created directory at: artifacts
[2024-09-05 22:03:51,554]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:03:56,285]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 22:03:56,294]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 22:03:56,341]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 22:03:56,341]: ERROR: main __init__() got an unexpected keyword argument 'use_ddp'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 58, in train
    trainer_args = Seq2SeqTrainingArguments(
TypeError: __init__() got an unexpected keyword argument 'use_ddp'
[2024-09-05 22:05:59,311]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:06:26,493]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:06:27,114]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:06:27,116]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:06:27,118]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:06:27,118]: INFO: common Created directory at: artifacts
[2024-09-05 22:06:27,118]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:06:32,341]: ERROR: main CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 76, in train
    trainer = Trainer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 545, in __init__
    self._move_model_to_device(model, args.device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 792, in _move_model_to_device
    model = model.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 22:06:56,965]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:06:57,585]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:06:57,587]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:06:57,588]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:06:57,588]: INFO: common Created directory at: artifacts
[2024-09-05 22:06:57,588]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:07:02,714]: ERROR: main CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 76, in train
    trainer = Trainer(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 545, in __init__
    self._move_model_to_device(model, args.device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 792, in _move_model_to_device
    model = model.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 14.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.48 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 992.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 22:12:01,614]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:12:02,291]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:12:02,293]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:12:02,294]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:12:02,294]: INFO: common Created directory at: artifacts
[2024-09-05 22:12:02,294]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:12:07,253]: ERROR: main CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 2.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.49 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 13.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 34, in train
    model_pegasus.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 2.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.49 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 13.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 22:13:21,988]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:13:22,664]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:13:22,667]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:13:22,668]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:13:22,668]: INFO: common Created directory at: artifacts
[2024-09-05 22:13:22,668]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:13:27,658]: ERROR: main CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 2.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.49 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 13.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 15, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 34, in train
    model_pegasus.to(device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2883, in to
    return super().to(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 21.96 GiB of which 2.69 MiB is free. Process 977795 has 20.45 GiB memory in use. Including non-PyTorch memory, this process has 1.49 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 13.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-05 22:54:08,138]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:54:08,757]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:54:08,760]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:54:08,761]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:54:08,761]: INFO: common Created directory at: artifacts
[2024-09-05 22:54:08,761]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:54:08,804]: ERROR: main train() missing 2 required positional arguments: 'rank' and 'world_size'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 17, in main
    torch.multiprocessing.spawn(model_trainer.train(), args=(world_size,), nprocs=world_size, join=True)
TypeError: train() missing 2 required positional arguments: 'rank' and 'world_size'
[2024-09-05 22:59:51,139]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 22:59:51,760]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 22:59:51,762]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 22:59:51,763]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 22:59:51,763]: INFO: common Created directory at: artifacts
[2024-09-05 22:59:51,763]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 22:59:51,807]: ERROR: main Default process group has not been initialized, please make sure to call init_process_group.
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 17, in main
    rank = torch.distributed.get_rank()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1809, in get_rank
    default_pg = _get_default_group()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1025, in _get_default_group
    raise ValueError(
ValueError: Default process group has not been initialized, please make sure to call init_process_group.
[2024-09-05 23:07:05,209]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 23:07:05,826]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 23:07:05,829]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 23:07:05,830]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 23:07:05,830]: INFO: common Created directory at: artifacts
[2024-09-05 23:07:05,830]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 23:07:05,874]: ERROR: main train() missing 2 required positional arguments: 'rank' and 'world_size'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 18, in main
    torch.multiprocessing.spawn(model_trainer.train(), args=(rank, world_size), nprocs=1)
TypeError: train() missing 2 required positional arguments: 'rank' and 'world_size'
[2024-09-05 23:07:12,003]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 23:07:12,620]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 23:07:12,623]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 23:07:12,624]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 23:07:12,624]: INFO: common Created directory at: artifacts
[2024-09-05 23:07:12,624]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 23:07:14,200]: INFO: config PyTorch version 2.4.0 available.
[2024-09-05 23:07:14,816]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-09-05 23:07:14,818]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-09-05 23:07:14,819]: INFO: common yaml file params.yaml loaded successfully.
[2024-09-05 23:07:14,819]: INFO: common Created directory at: artifacts
[2024-09-05 23:07:14,819]: INFO: common Created directory at: artifacts/model_trainer
[2024-09-05 23:07:14,863]: ERROR: main 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/Text-Summarizer/main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 18, in main
    torch.multiprocessing.spawn(model_trainer.train, args=(rank, world_size), nprocs=1)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 282, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 229, in start_processes
    process.start()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 42, in _launch
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/spawn.py", line 154, in get_preparation_data
    _check_not_importing_main()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/spawn.py", line 134, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
[2024-09-05 23:07:15,238]: ERROR: main process 0 terminated with exit code 1
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 18, in main
    torch.multiprocessing.spawn(model_trainer.train, args=(rank, world_size), nprocs=1)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 282, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 238, in start_processes
    while not context.join():
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 178, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1
[2024-11-03 09:59:06,708]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 09:59:07,720]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 09:59:07,722]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 09:59:07,723]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 09:59:07,724]: INFO: common Created directory at: artifacts
[2024-11-03 09:59:07,724]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 09:59:09,307]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 09:59:09,948]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 09:59:09,950]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 09:59:09,951]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 09:59:09,951]: INFO: common Created directory at: artifacts
[2024-11-03 09:59:09,951]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 09:59:10,000]: ERROR: main 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
Traceback (most recent call last):
  File "/home/ssehg1@cfreg.local/Text-Summarizer/main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 18, in main
    torch.multiprocessing.spawn(model_trainer.train, args=(rank, world_size), nprocs=1)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 282, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 229, in start_processes
    process.start()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 42, in _launch
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/spawn.py", line 154, in get_preparation_data
    _check_not_importing_main()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/multiprocessing/spawn.py", line 134, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
[2024-11-03 09:59:10,388]: ERROR: main process 0 terminated with exit code 1
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 18, in main
    torch.multiprocessing.spawn(model_trainer.train, args=(rank, world_size), nprocs=1)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 282, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 238, in start_processes
    while not context.join():
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 178, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 1
[2024-11-03 10:02:55,071]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:02:56,071]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-11-03 10:02:56,073]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:02:56,074]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:02:56,075]: INFO: common Created directory at: artifacts
[2024-11-03 10:02:56,075]: INFO: common Created directory at: artifacts/data_ingestion
[2024-11-03 10:02:56,075]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-11-03 10:02:56,185]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-11-03 10:02:56,185]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-11-03 10:02:56,187]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:02:56,188]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:02:56,188]: INFO: common Created directory at: artifacts
[2024-11-03 10:02:56,189]: INFO: common Created directory at: artifacts/data_validation
[2024-11-03 10:02:56,189]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-11-03 10:02:56,189]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-11-03 10:02:56,191]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:02:56,192]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:02:56,192]: INFO: common Created directory at: artifacts
[2024-11-03 10:02:56,192]: INFO: common Created directory at: artifacts/data_transformation
[2024-11-03 10:02:57,316]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-11-03 10:02:57,316]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:02:57,319]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:02:57,320]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:02:57,320]: INFO: common Created directory at: artifacts
[2024-11-03 10:02:57,320]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 10:03:02,589]: ERROR: main 'PegasusTokenizerFast' object has no attribute 'to'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 16, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 32, in train
    tokenizer.to(device)
AttributeError: 'PegasusTokenizerFast' object has no attribute 'to'
[2024-11-03 10:04:50,090]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:04:51,104]: INFO: main >>>>>>>>>> stage Data Ingestion Stage started <<<<<<<<<
[2024-11-03 10:04:51,106]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:04:51,108]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:04:51,108]: INFO: common Created directory at: artifacts
[2024-11-03 10:04:51,108]: INFO: common Created directory at: artifacts/data_ingestion
[2024-11-03 10:04:51,108]: INFO: data_ingestion File already exists of size: ~ 7718 KB
[2024-11-03 10:04:51,220]: INFO: main >>>>>>>>>> stage Data Ingestion Stage completed <<<<<<<<<

x=============x
[2024-11-03 10:04:51,220]: INFO: main >>>>>>>>>> stage Data Validation Stage started <<<<<<<<<
[2024-11-03 10:04:51,222]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:04:51,223]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:04:51,223]: INFO: common Created directory at: artifacts
[2024-11-03 10:04:51,223]: INFO: common Created directory at: artifacts/data_validation
[2024-11-03 10:04:51,223]: INFO: main >>>>>>>>>> stage Data Validation Stage completed <<<<<<<<<

x=============x
[2024-11-03 10:04:51,223]: INFO: main >>>>>>>>>> stage Data Transformation Stage started <<<<<<<<<
[2024-11-03 10:04:51,226]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:04:51,227]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:04:51,227]: INFO: common Created directory at: artifacts
[2024-11-03 10:04:51,227]: INFO: common Created directory at: artifacts/data_transformation
[2024-11-03 10:04:52,069]: INFO: main >>>>>>>>>> stage Data Transformation Stage completed <<<<<<<<<

x=============x
[2024-11-03 10:04:52,069]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:04:52,072]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:04:52,073]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:04:52,073]: INFO: common Created directory at: artifacts
[2024-11-03 10:04:52,073]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 10:04:57,268]: ERROR: main 'DatasetDict' object has no attribute 'to'
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 16, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 33, in train
    dataset_samsum_pt = dataset_samsum_pt.to(device)
AttributeError: 'DatasetDict' object has no attribute 'to'
[2024-11-03 10:05:25,724]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:05:26,734]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:05:26,737]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:05:26,738]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:05:26,738]: INFO: common Created directory at: artifacts
[2024-11-03 10:05:26,738]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 10:06:41,690]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:06:42,745]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:06:42,748]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:06:42,749]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:06:42,749]: INFO: common Created directory at: artifacts
[2024-11-03 10:06:42,749]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 10:07:54,662]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:07:55,758]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:07:55,760]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:07:55,761]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:07:55,761]: INFO: common Created directory at: artifacts
[2024-11-03 10:07:55,761]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 10:08:06,690]: ERROR: main CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.17 GiB is free. Including non-PyTorch memory, this process has 19.78 GiB memory in use. Of the allocated memory 19.43 GiB is allocated by PyTorch, and 31.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 16, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 63, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 187, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 204, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 109, in gather
    res = gather_map(outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/utils/generic.py", line 390, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 94, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.17 GiB is free. Including non-PyTorch memory, this process has 19.78 GiB memory in use. Of the allocated memory 19.43 GiB is allocated by PyTorch, and 31.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-11-03 10:08:21,558]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:08:22,607]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:08:22,610]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:08:22,611]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:08:22,611]: INFO: common Created directory at: artifacts
[2024-11-03 10:08:22,611]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 10:10:15,149]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:10:16,151]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:10:16,153]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:10:16,154]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:10:16,155]: INFO: common Created directory at: artifacts
[2024-11-03 10:10:16,155]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 10:10:28,010]: ERROR: main CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.17 GiB is free. Including non-PyTorch memory, this process has 19.78 GiB memory in use. Of the allocated memory 19.43 GiB is allocated by PyTorch, and 31.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 16, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 64, in train
    trainer.train()
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
    return inner_training_loop(
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 187, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 204, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 109, in gather
    res = gather_map(outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/utils/generic.py", line 390, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 100, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py", line 94, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 21.96 GiB of which 2.17 GiB is free. Including non-PyTorch memory, this process has 19.78 GiB memory in use. Of the allocated memory 19.43 GiB is allocated by PyTorch, and 31.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-11-03 10:14:37,692]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:14:38,728]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:14:38,730]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:14:38,731]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:14:38,732]: INFO: common Created directory at: artifacts
[2024-11-03 10:14:38,732]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 10:14:43,956]: ERROR: main --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: epoch
- Save strategy: steps
Traceback (most recent call last):
  File "main.py", line 48, in <module>
    model_training.main()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/pipeline/stage_04_model_trainer.py", line 16, in main
    model_trainer.train()
  File "/home/ssehg1@cfreg.local/Text-Summarizer/src/textSummarizer/components/model_trainer.py", line 36, in train
    trainer_args = TrainingArguments(
  File "<string>", line 131, in __init__
  File "/home/ssehg1@cfreg.local/.conda/envs/textS/lib/python3.8/site-packages/transformers/training_args.py", line 1593, in __post_init__
    raise ValueError(
ValueError: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: epoch
- Save strategy: steps
[2024-11-03 10:15:14,826]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:15:15,833]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:15:15,836]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:15:15,837]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:15:15,837]: INFO: common Created directory at: artifacts
[2024-11-03 10:15:15,837]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 10:19:25,624]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 10:19:26,674]: INFO: main >>>>>>>>>> stage Model Training Stage started <<<<<<<<<
[2024-11-03 10:19:26,677]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 10:19:26,678]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 10:19:26,678]: INFO: common Created directory at: artifacts
[2024-11-03 10:19:26,678]: INFO: common Created directory at: artifacts/model_trainer
[2024-11-03 11:55:44,704]: INFO: main >>>>>>>>>> stage Model Training Stage completed <<<<<<<<<

x=============x
[2024-11-03 12:53:29,518]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 12:53:30,519]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-11-03 12:53:30,522]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 12:53:30,523]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 12:53:30,523]: INFO: common Created directory at: artifacts
[2024-11-03 12:53:30,523]: INFO: common Created directory at: artifacts/model_evaluation
[2024-11-03 12:53:36,154]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:37,320]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:38,705]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:39,813]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:41,015]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:42,275]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:43,501]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:44,784]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:45,990]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:47,984]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:50,057]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:51,209]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:52,580]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:53,828]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:55,133]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:56,839]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:58,004]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:53:59,272]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:00,478]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:01,842]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:03,263]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:04,547]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:05,833]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:07,180]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:08,408]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:09,615]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:10,824]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:11,970]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:13,258]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:14,429]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:15,577]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:16,827]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:18,231]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:19,365]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:20,555]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:21,726]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:22,969]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:24,399]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:25,665]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:27,021]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:28,211]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:29,343]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:30,576]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:31,805]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:32,956]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:34,302]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:35,450]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:36,661]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:37,872]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:39,003]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:40,336]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:41,468]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:42,721]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:43,876]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:45,467]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:46,699]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:47,939]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:49,275]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:50,406]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:52,561]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:53,803]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:54,956]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:56,167]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:57,339]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:54:58,492]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:00,063]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:01,626]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:02,760]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:03,835]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:04,990]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:06,182]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:07,334]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:08,724]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:10,115]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:11,251]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:12,431]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:14,013]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:15,689]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:17,391]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:18,526]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:19,596]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:20,687]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:22,347]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:23,781]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:25,673]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:26,947]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:28,186]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:29,298]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:30,488]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:31,681]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:33,253]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:34,509]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:35,582]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:36,889]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:38,162]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:39,677]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:41,286]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:42,955]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:44,129]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:45,242]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:46,378]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:48,093]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:49,371]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:50,547]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:52,018]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:53,255]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:54,426]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:56,138]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:57,571]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:55:58,921]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:00,056]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:01,754]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:02,974]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:04,188]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:05,419]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:06,609]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:07,739]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:09,034]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:10,287]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:11,505]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:12,676]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:14,417]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:15,633]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:17,053]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:18,228]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:19,481]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:20,690]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:22,065]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:23,363]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:24,723]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:25,973]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:27,269]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:28,501]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:29,854]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:31,083]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:32,273]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:33,425]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:34,537]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:36,232]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:37,467]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:38,858]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:40,410]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:41,566]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:43,085]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:44,619]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:46,218]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:47,539]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:48,652]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:49,825]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:51,057]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:52,311]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:54,340]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:55,920]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:57,195]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:56:59,080]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:00,617]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:01,830]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:03,079]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:04,276]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:05,704]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:06,971]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:08,242]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:09,414]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:10,726]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:12,338]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:13,629]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:14,839]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:16,072]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:17,249]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:18,462]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:19,836]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:21,513]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:22,608]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:23,820]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:24,928]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:27,130]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:28,350]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:29,465]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:30,712]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:32,243]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:33,519]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:34,833]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:36,119]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:37,211]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:38,322]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:39,860]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:41,377]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:42,837]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:44,049]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:45,279]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:46,847]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:48,020]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:49,159]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:50,308]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:51,763]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:53,944]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:55,056]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:56,227]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:57,523]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:57:59,305]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:00,539]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:01,929]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:03,221]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:04,840]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:05,994]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:07,163]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:09,226]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:11,124]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:12,279]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:13,548]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:14,982]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:16,378]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:17,510]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:18,809]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:20,239]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:22,016]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:23,213]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:24,756]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:26,064]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:27,277]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:28,447]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:29,558]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:31,045]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:32,541]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:33,830]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:35,262]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:36,433]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:37,605]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:38,822]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:40,103]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:41,497]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:42,630]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:43,803]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:45,177]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:46,329]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:47,599]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:48,850]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:49,964]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:51,379]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:52,557]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:53,753]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:55,285]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:56,418]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:57,547]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:58,740]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:58:59,957]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:01,110]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:02,420]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:03,634]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:04,844]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:06,161]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:07,334]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:08,950]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:10,727]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:11,859]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:12,971]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:14,142]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:15,354]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:16,508]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:17,835]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:19,029]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:20,217]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:21,470]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:23,149]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:24,525]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:26,763]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:28,016]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:29,186]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:30,259]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:31,506]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:32,859]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:34,251]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:35,366]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:36,556]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:37,726]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:38,921]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:40,225]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:41,538]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:43,131]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:44,446]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:45,620]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:46,731]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:47,983]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:49,397]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:50,526]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:51,817]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:53,509]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:54,746]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:55,920]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:57,251]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:58,403]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 12:59:59,833]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:01,770]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:03,004]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:04,621]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:05,898]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:07,133]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:08,329]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:09,442]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:10,653]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:11,887]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:13,001]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:14,389]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:15,935]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:17,133]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:18,250]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:19,445]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:20,637]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:22,233]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:23,466]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:24,678]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:26,251]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:27,981]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:29,399]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:30,590]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:31,705]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:33,620]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:34,951]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:36,164]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:37,357]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:38,551]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:39,760]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:40,891]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:42,123]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:43,357]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:44,487]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:45,682]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:48,681]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:49,778]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:50,986]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:52,317]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:53,564]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:54,893]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:56,183]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:57,654]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:00:59,110]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:00,441]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:01,818]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:03,113]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:04,975]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:06,532]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:07,804]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:09,012]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:10,297]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:11,611]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:13,097]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:14,273]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:15,829]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:17,398]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:18,727]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:20,358]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:21,753]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:23,408]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:24,797]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:25,987]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:27,118]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:28,288]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:29,985]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:31,154]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:32,247]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:33,322]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:34,788]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:35,942]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:37,492]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:38,922]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:40,075]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:41,263]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:42,574]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:43,805]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:45,058]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:46,209]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:47,524]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:48,917]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:50,185]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:51,379]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:52,973]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:54,207]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:55,378]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:56,546]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:57,719]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:58,874]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:01:59,986]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:01,678]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:02,898]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:04,193]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:05,406]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:06,996]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:08,270]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:09,443]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:11,238]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:12,452]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:13,584]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:14,830]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:16,857]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:18,110]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:19,320]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:20,433]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:21,931]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:23,008]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:24,215]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:25,446]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:26,740]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:28,030]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:29,184]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:30,354]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:31,506]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:32,697]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:33,848]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:34,986]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:35,863]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 13:02:35,968]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
[2024-11-03 14:00:14,717]: INFO: config PyTorch version 2.4.0 available.
[2024-11-03 14:00:15,742]: INFO: main >>>>>>>>>> stage Model Evaluation Stage started <<<<<<<<<
[2024-11-03 14:00:15,745]: INFO: common yaml file config/config.yaml loaded successfully.
[2024-11-03 14:00:15,746]: INFO: common yaml file params.yaml loaded successfully.
[2024-11-03 14:00:15,747]: INFO: common Created directory at: artifacts
[2024-11-03 14:00:15,747]: INFO: common Created directory at: artifacts/model_evaluation
[2024-11-03 14:00:24,210]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:00:27,935]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:00:33,816]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:00:38,696]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:00:42,627]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:00:46,727]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:00:50,292]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:00:54,016]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:00:58,058]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:02,175]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:05,756]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:09,639]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:13,469]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:18,013]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:24,070]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:27,668]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:32,174]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:35,640]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:39,659]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:44,515]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:49,257]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:54,694]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:01:59,198]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:03,526]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:08,247]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:13,123]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:18,005]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:22,839]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:26,442]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:30,182]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:35,127]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:39,089]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:43,007]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:46,909]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:51,716]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:02:56,183]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:00,735]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:06,458]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:11,807]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:15,931]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:19,735]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:24,353]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:29,142]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:35,327]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:39,722]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:43,543]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:47,973]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:52,489]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:03:58,629]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:03,693]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:08,327]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:14,174]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:18,323]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:23,402]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:27,824]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:32,138]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:36,359]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:40,380]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:44,359]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:48,446]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:52,828]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:04:56,643]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:00,464]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:05,516]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:09,385]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:14,161]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:20,461]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:24,478]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:27,974]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:32,547]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:36,625]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:41,462]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:45,595]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:51,111]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:54,727]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:05:59,128]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:02,625]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:07,198]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:12,100]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:17,531]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:21,140]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:29,554]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:33,411]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:37,669]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:42,963]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:47,415]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:51,872]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:06:56,556]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:01,280]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:06,094]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:10,550]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:14,692]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:18,518]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:23,082]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:26,682]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:31,498]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:36,058]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:41,147]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:46,868]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:51,147]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:54,976]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:07:58,921]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:08:00,767]: INFO: rouge_scorer Using default tokenizer.
[2024-11-03 14:08:00,880]: INFO: main >>>>>>>>>> stage Model Evaluation Stage completed <<<<<<<<<

x=============x
